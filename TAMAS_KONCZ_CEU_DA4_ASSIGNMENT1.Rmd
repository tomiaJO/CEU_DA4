---
title: "DA4 -  Assignment 1"
author: "Tamas Koncz"
date: '2018-02-11'
output:
  html_document:
    df_print: paged
  html_notebook:
    df_print: paged
  pdf_document: default
---

```{r setup, message=FALSE, include=FALSE}
require(data.table)
require(ggplot2)
require(gridExtra)
require(caret)

require(knitr)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)

theme_set(theme_minimal())   # globally set ggplot theme

set.seed(1234)
RMSE <- function(x, true_x) sqrt(mean((x - true_x)^2))
ERR_SQ <- function(x, true_x) (x - true_x)^2
```


```{r, include= FALSE}
data <- fread("airbnb_london_workfile.csv",
              stringsAsFactors = FALSE)
```
 
 
 
 
  
  
#### Code snippet for selecting random borough 

Below code was used to pick a random borough from the dataset.
Here, and in later parts as well, where a random generator is involved in producing the results, I used set.seed(93) to help foster reproducibility of results.
```{r}
boroughs <- data[, .(count = .N, 
                     avg_price = mean(price)), 
                 keyby = f_neighbourhood_cleansed][order(-count)]

boroughs[, borough := factor(f_neighbourhood_cleansed, 
                             levels = boroughs[order(count)][, f_neighbourhood_cleansed])]
boroughs[, f_neighbourhood_cleansed := NULL]

# randomly picking an area > 1000
set.seed(93) #for reproducibility
selected <- sample(boroughs[count > 1000]$borough, 1)

```


```{r, fig.align= 'center', fig.width= 10, echo= FALSE}
max_count <- boroughs[, max(count)]
max_avg_price <- boroughs[, max(avg_price)]

boroughs[borough == selected, ] #TODO: make this bold on the chart

ggplot(data = boroughs) + 
  geom_bar(data = boroughs[borough == selected], aes(x = borough, y = 6000), fill = "lightblue",  stat = "identity") +
  geom_point(aes(x = borough, y = count, color = "# of Observations"), shape = 20, size = 2) +
  geom_segment(aes(x = borough, y = count, xend = borough, yend =0, color = "# of Observations")) + 
  geom_point(aes(x = borough, y = avg_price * (max_count/max_avg_price), color = "Avg. Price"), shape = 4, size = 2) + 
  scale_y_continuous(limits = c(0,6000), sec.axis = sec_axis(~./(max_count/max_avg_price), name = "Avg. Price")) + 
  scale_color_manual(name = "Legend", values = c("# of Observations" = "tomato", "Avg. Price" = "darkblue")) + 
  guides(color=guide_legend(override.aes=list(shape=15))) +
  labs(y = "# of Observations", x = "Borough") +
  coord_flip()
```

```{r, include= FALSE}
london <- copy(data)
rm(data)
rm(boroughs)
rm(max_count)
rm(max_avg_price)
```

#### Data cleaning

The below steps were applied to prepare the data for analysis.

    1. Handling missing (NA) values: 
```{r, echo= FALSE}
#handling NAs
missing_values <- as.data.table(t(london[, lapply(.SD, function(x) sum(is.na(x))), .SDcols = names(london)]), keep.rownames=TRUE)
setnames(missing_values, c("variable", "NA.Count"))

missing_values[order(-NA.Count)][NA.Count>0]
```

I followed the logic of checking if there seems to be any significant relationship between variables with many NA-s and price (example graph below).
As I did not see anything meaningful, I decided to drop these variables. There might be a way to guess their values based on the data, however, given the lack of expected predicting power, I took the "clean" path.

```{r, echo= FALSE, fig.height=3, fig.align='center', fig.width=4}
ggplot(data= london, aes(x = n_review_scores_rating, y = price)) + 
  geom_point() + 
  geom_smooth(method='lm')
```

Upon checking airbnb.com, I decided that the missing cleaning fees _actually_ mean 0 added cost for this item, hence I replaced NAs with zero for this one variable:
```{r}
london[, cleaning_fee := ifelse(is.na(usd_cleaning_fee), 0, usd_cleaning_fee)]
```

And then just proceeded to drop the many-NA variables:
```{r}
london[, c("usd_cleaning_fee", 
           "n_review_scores_rating",
           "n_reviews_per_month",
           "n_days_since",
           "p_host_response_rate") := NULL]
```

We are almost done - for the reamining two variables with some NAs, I was comfortable dropping the whole observation row, as their numbers are very limited compared to the whole dataset:
```{r}
london <- london[complete.cases(london)]
```

    2. Getting rid of unnecessary variables

There were some variables which encoded duplicates, I just dropped all these values:
```{r}
london[, c("neighbourhood_cleansed", 
           "property_type", 
           "room_type", 
           "usd_price_day", 
           "cancellation_policy") := NULL]
```


```{r, include= FALSE}
factor_cols <- names(london)[names(london) %like% "^f_.*"]
london[, (factor_cols) := lapply(.SD, as.factor), .SDcols = factor_cols]
```

    3. Feature engineering
The last step of data preparation was creating a few set of additional variables that I expect to improve our models' fit.

The first one was to create log(...) transformed version of price - this will be explained in detail in the next section.
```{r}
london[, log_price:= log(price)]
```

I've created another variable for the dummies - this one however is a total "score" (summing up all available extras).  
This is a very simple approach, without any weightings on different benefits - however, it can provide some insight into how people value non-core offerings.
```{r}
london[, d_total := Reduce("+", .SD), .SDcols = names(london) %like% "^d_.*"]
```

For some variables, I've added second polinomial terms, for better fit. It will be visible in the next session that these variables bear a somewhat non-linear relationship to log(price).
```{r}
london[, cleaning_fee_sq:= cleaning_fee^2]
london[, n_accommodates_sq:= n_accommodates^2]
london[, n_beds_sq:= n_beds^2]
```


```{r, include = FALSE}
rm(missing_values)
```

```{r, include = FALSE}
#creating subset
kensington_chelsea <- london[f_neighbourhood_cleansed == selected]
```

#### Data exploration  
  
Looking at the prices variable, we can see that it's close to being lognormally distributed.
Although log-level predictions is not without difficulties, I chose to still apply this transformation, and use log(price) is my linear models, in hope of a better fit. Corrections for the log transformation based predictions will be explained later.
```{r, echo= FALSE, fig.width= 15}
p1 <- ggplot(data= london, aes(x = price)) + geom_histogram() + labs(title= "Full London")
p2 <- ggplot(data= kensington_chelsea, aes(x = price)) + geom_histogram() + labs(title= "Kensington and Chelsea")
p3 <- ggplot(data= london, aes(x = log_price)) + geom_histogram()
p4 <- ggplot(data= kensington_chelsea, aes(x = log_price)) + geom_histogram()

grid.arrange(p1, p2, p3, p4, ncol = 2)
```

I'm plotting several of the important variables below (distribution & price for both the full London dataset and for the subsample as well). I'm only going to comment on the ones which have additional consequence.  
  
n_accommodates:
```{r, echo= FALSE, fig.width= 15}
p1 <- ggplot(data= london, aes(x = n_accommodates)) + geom_histogram() + labs(title= "Full London")
p2 <- ggplot(data= kensington_chelsea, aes(x = n_accommodates)) + geom_histogram() + labs(title= "Kensington and Chelsea")
p3 <- ggplot(data= london, aes(x = n_accommodates, y = log_price)) + geom_point() + geom_smooth()
p4 <- ggplot(data= kensington_chelsea, aes(x = n_accommodates, y = log_price)) + geom_point() + geom_smooth()

grid.arrange(p1, p2, p3, p4, ncol = 2)

#transform for diminishing returns?
```
  
  
n_beds:
```{r, echo= FALSE, fig.width= 15}
p1 <- ggplot(data= london, aes(x = n_beds)) + geom_histogram() + labs(title= "Full London")
p2 <- ggplot(data= kensington_chelsea, aes(x = n_beds)) + geom_histogram() + labs(title= "Kensington and Chelsea")
p3 <- ggplot(data= london, aes(x = n_beds, y = log_price)) + geom_point() + geom_smooth()
p4 <- ggplot(data= kensington_chelsea, aes(x = n_beds, y = log_price)) + geom_point() + geom_smooth()

grid.arrange(p1, p2, p3, p4, ncol = 2)

#transform for diminishing returns?
```
  
  
f_room_type:
```{r, echo= FALSE, fig.width= 15}
p1 <- ggplot(data= london, aes(x = f_room_type)) + geom_bar() + labs(title= "Full London")
p2 <- ggplot(data= kensington_chelsea, aes(x = f_room_type)) + geom_bar() + labs(title= "Kensington and Chelsea")
p3 <- ggplot(data= london, aes(x = f_room_type, y = log_price)) + geom_boxplot()
p4 <- ggplot(data= kensington_chelsea, aes(x = f_room_type, y = log_price)) + geom_boxplot()

grid.arrange(p1, p2, p3, p4, ncol = 2)
```
  
  
f_cancellation_policy:
```{r, echo= FALSE, fig.width= 15}
p1 <- ggplot(data= london, aes(x = f_cancellation_policy)) + geom_bar() + labs(title= "Full London")
p2 <- ggplot(data= kensington_chelsea, aes(x = f_cancellation_policy)) + geom_bar() + labs(title= "Kensington and Chelsea")
p3 <- ggplot(data= london, aes(x = f_cancellation_policy, y = log_price)) + geom_boxplot()
p4 <- ggplot(data= kensington_chelsea, aes(x = f_cancellation_policy, y = log_price)) + geom_boxplot()

grid.arrange(p1, p2, p3, p4, ncol = 2)

# f_cancellation_policy -> surprising result. correlation with sth else?
```
  
n_number_of_reviews:
```{r, echo= FALSE, fig.width= 15}
p1 <- ggplot(data= london, aes(x = n_number_of_reviews)) + geom_histogram() + labs(title= "Full London")
p2 <- ggplot(data= kensington_chelsea, aes(x = n_number_of_reviews)) + geom_histogram() + labs(title= "Kensington and Chelsea")
p3 <- ggplot(data= london, aes(x = n_number_of_reviews, y = log_price)) + geom_point() + geom_smooth()
p4 <- ggplot(data= kensington_chelsea, aes(x = n_number_of_reviews, y = log_price)) + geom_point() + geom_smooth()

grid.arrange(p1, p2, p3, p4, ncol = 2)
```  
  
n_minimum_nights:
```{r, echo= FALSE, fig.width= 15}
p1 <- ggplot(data= london, aes(x = n_minimum_nights)) + geom_histogram() + labs(title= "Full London")
p2 <- ggplot(data= kensington_chelsea, aes(x = n_minimum_nights)) + geom_histogram() + labs(title= "Kensington and Chelsea")
p3 <- ggplot(data= london, aes(x = n_minimum_nights, y = log_price)) + geom_point() + geom_smooth()
p4 <- ggplot(data= kensington_chelsea, aes(x = n_minimum_nights, y = log_price)) + geom_point() + geom_smooth()

grid.arrange(p1, p2, p3, p4, ncol = 2)
```  
  
f_cancellation_policy:
```{r, echo= FALSE, fig.width= 15}
p1 <- ggplot(data= london, aes(x = f_property_type)) + geom_bar() + labs(title= "Full London")
p2 <- ggplot(data= kensington_chelsea, aes(x = f_property_type)) + geom_bar() + labs(title= "Kensington and Chelsea")
p3 <- ggplot(data= london, aes(x = f_property_type, y = log_price)) + geom_boxplot()
p4 <- ggplot(data= kensington_chelsea, aes(x = f_property_type, y = log_price)) + geom_boxplot()

grid.arrange(p1, p2, p3, p4, ncol = 2)
```

cleaning_fee:
```{r, echo= FALSE, fig.width= 15}
p1 <- ggplot(data= london, aes(x = cleaning_fee)) + geom_histogram() + labs(title= "Full London")
p2 <- ggplot(data= kensington_chelsea, aes(x = cleaning_fee)) + geom_histogram() + labs(title= "Kensington and Chelsea")
p3 <- ggplot(data= london, aes(x = cleaning_fee, y = log_price)) + geom_point() + geom_smooth()
p4 <- ggplot(data= kensington_chelsea, aes(x = cleaning_fee, y = log_price)) + geom_point() + geom_smooth()

grid.arrange(p1, p2, p3, p4, ncol = 2)
```  

I'm sure people will be happy to pay more for a nice breakfast:
```{r, echo= FALSE, fig.width= 15}
p1 <- ggplot(data= london, aes(x = factor(d_breakfast))) + geom_bar() + labs(title= "Full London")
p2 <- ggplot(data= kensington_chelsea, aes(x = factor(d_breakfast))) + geom_bar() + labs(title= "Kensington and Chelsea")
p3 <- ggplot(data= london, aes(x = factor(d_breakfast), y = log_price)) + geom_boxplot()
p4 <- ggplot(data= kensington_chelsea, aes(x = factor(d_breakfast), y = log_price)) + geom_boxplot()

grid.arrange(p1, p2, p3, p4, ncol = 2)
```  
Well, maybe not so much...  
What about other features?  
  
Looking at the earlier created feature-score, d_total, we will see a slight positive impact:
```{r, echo= FALSE, fig.width= 15}
p1 <- ggplot(data= london, aes(x = d_total)) + geom_histogram() + labs(title= "Full London")
p2 <- ggplot(data= kensington_chelsea, aes(x = d_total)) + geom_histogram() + labs(title= "Kensington and Chelsea")
p3 <- ggplot(data= london, aes(x = d_total, y = log_price)) + geom_point() + geom_smooth()
p4 <- ggplot(data= kensington_chelsea, aes(x = d_total, y = log_price)) + geom_point() + geom_smooth()

grid.arrange(p1, p2, p3, p4, ncol = 2)
```

#### Creating a hold-out test set

I'm create separate datasets for model training and performance evaluations (same method was followed for the Kensington and Chelsea subsample), using basic Caret functions.  
Model selection will be done via cross-validation on the training set.
```{r}
training_ratio <- 0.7

set.seed(93) #for reproducibility
train_indices <- createDataPartition(y = london[["log_price"]],
                                     times = 1,
                                     p = training_ratio,
                                     list = FALSE)
london_train <- london[train_indices, ]
london_test <- london[-train_indices, ]
```

```{r, include= FALSE}
set.seed(93) #for reproducibility
train_indices <- createDataPartition(y = kensington_chelsea[["log_price"]],
                                     times = 1,
                                     p = training_ratio,
                                     list = FALSE)
kensington_chelsea_train <- kensington_chelsea[train_indices, ]
kensington_chelsea_test <- kensington_chelsea[-train_indices, ]
```
 
 
#### Setting the control parameters for 10-fold CV  

```{r}
fit_control <- trainControl(method = "cv", number = 10)
```

#### Model training

I'm fitting four models, twice each time - they share the same predictors and train(...) parameters. Hence, I'll only include code for the whole London dataset - what happens for the subsample is analogous each time.  
  
The first model is very basic - it just uses one predictor, n_accommodates. It is expected to be used rather as a benchmark, than an actual model for prediction.
```{r}
set.seed(93) #for reproducibility
model_1_london <- train(log_price ~ n_accommodates, 
                   data = london_train, 
                   method = "lm", 
                   trControl = fit_control)
```

```{r, include= FALSE}
set.seed(93) #for reproducibility
model_1_kensington_chelsea <- train(log_price ~ n_accommodates, 
                   data = kensington_chelsea_train, 
                   method = "lm", 
                   trControl = fit_control)
```


The second model I'm going to train is somewhat more complicated, still linear, but using multiple regression.  
The variables I've selected are based on what I think could be explanatory of price from the business perspective (e.g. an entire apartmant should be higher priced than just a room).  
  
It's important to highlight one variable I'm not using, which is location information (borough).  
This is just because my interpretation of the task, to be able to use the exact same models for full London and the subsample (so for Full London, this might not be optimal for prediction power - in a project with actual dollar-impact my decision would have been obviously different).  

```{r}
set.seed(93) #for reproducibility
model_2_london <- train(log_price ~ n_accommodates + n_beds + n_bathrooms + 
                          f_property_type + f_room_type + f_bed_type, 
                   data = london_train, 
                   method = "lm", 
                   trControl = fit_control)
```

```{r, include= FALSE}

set.seed(93) #for reproducibility
model_2_kensington_chelsea <- train(log_price ~ n_accommodates + n_beds + n_bathrooms +
                                            f_property_type + f_room_type + f_bed_type, 
                                          data = kensington_chelsea_train, 
                                          method = "lm",
                                          trControl = fit_control)
```
  
  
  
The third model is a "one step further" version of the previous one.  
Two important differences are:  
    a. adding second polynomial terms for some variables, where the relationship to price is expected to be non-linear
    b. use of "d_total", which is the unweighted score created previously for all the feature-dummies

```{r}
#complex model

set.seed(93) #for reproducibility
model_3_london <- train(log_price ~ n_accommodates + n_accommodates_sq + n_beds + n_beds_sq + n_bathrooms + 
                          cleaning_fee + cleaning_fee_sq + f_property_type + f_room_type + f_bed_type + d_total, 
                   data = london_train, 
                   method = "lm", 
                   trControl = fit_control)
```

```{r, include= FALSE}

set.seed(93) #for reproducibility
model_3_kensington_chelsea <- train(log_price ~ n_accommodates + n_accommodates_sq + n_beds + n_beds_sq +
                                      n_bathrooms + cleaning_fee + cleaning_fee_sq + f_property_type + 
                                      f_room_type + f_bed_type + d_total, 
                                          data = kensington_chelsea_train, 
                                          method = "lm",
                                          trControl = fit_control)
```

Finally, the last model will use all variables that can be input to both dataset (some had to be filtered out, as they take only one level in the subsample).   
Generally speaking, using all variables, with this many being available, could risk overfitting the parameters.  
To control for this, I'm using LASSO-regularization - a technique that essentially penalizes coefficients getting too large in the model, making the model less likely to overfit.  
  
Setting up LASSO, and hyperparameter-tuning with Caret:  
```{r}
tune_grid <- expand.grid("alpha" = 1,
                             "lambda" = seq(0, 0.015, 0.001))
```

    
```{r, include= FALSE}
l <- sapply(kensington_chelsea_train, function(x) is.factor(x))
m <- kensington_chelsea_train[, ..l]

sapply(kensington_chelsea_train, function(x) length(unique(x)))

rm(l)
rm(m)
```

  
And then calling the train function:    

```{r}
#full model - most complexity, all variables

set.seed(93) #for reproducibility
model_4_london <- train(log_price ~ . -price -d_total -f_neighbourhood_cleansed -d_washerdryer -d_freeparkingonstreet -d_paidparkingoffpremises, 
                    data = london_train, 
                    method = "glmnet",
                    preProcess = c("center", "scale"),
                    tuneGrid = tune_grid,
                    metric = "RMSE",
                    trControl = fit_control)
```


```{r}
#full model - most complexity, all variables

set.seed(93) #for reproducibility
model_4_kensington_chelsea <- train(log_price ~ . -price -d_total -f_neighbourhood_cleansed -d_washerdryer -d_freeparkingonstreet -d_paidparkingoffpremises, 
                    data = kensington_chelsea_train, 
                    method = "glmnet",
                    preProcess = c("center", "scale"),
                    tuneGrid = tune_grid,
                    metric = "RMSE",
                    trControl = fit_control)
```

#### Selecting the best model

RMSE values from the different models are below:
```{r, echo= FALSE, fig.align= 'center', fig.width= 7.5, fig.height= 5}
model_1_london_rmse_cv <- model_1_london$results[["RMSE"]]
model_2_london_rmse_cv <- model_2_london$results[["RMSE"]]
model_3_london_rmse_cv <- model_3_london$results[["RMSE"]]
model_4_london_rmse_cv <- min(model_4_london$results[["RMSE"]])

model_1_kensington_chelsea_rmse_cv <- model_1_kensington_chelsea$results[["RMSE"]]
model_2_kensington_chelsea_rmse_cv <- model_2_kensington_chelsea$results[["RMSE"]]
model_3_kensington_chelsea_rmse_cv <- model_3_kensington_chelsea$results[["RMSE"]]
model_4_kensington_chelsea_rmse_cv <- min(model_4_kensington_chelsea$results[["RMSE"]])

m_logrmse <- matrix(c(1, model_1_london_rmse_cv, model_1_kensington_chelsea_rmse_cv,
                   2, model_2_london_rmse_cv, model_2_kensington_chelsea_rmse_cv,
                   3, model_3_london_rmse_cv, model_3_kensington_chelsea_rmse_cv,
                   4, model_4_london_rmse_cv, model_4_kensington_chelsea_rmse_cv), 
                 nrow= 4, byrow= TRUE)
colnames(m_logrmse) <- c('model', 'London RMSE', "Kensington and Chelsea RMSE")
logrmse.table <- data.table(m_logrmse)

ggplot(data= logrmse.table) +
  geom_point(aes(x= model, y= as.numeric(`London RMSE`), color= "London"), stat="identity") +
  geom_line(aes(x= model, y= as.numeric(`London RMSE`), color= "London"), stat="identity") +
  geom_point(aes(x= model, y= as.numeric(`Kensington and Chelsea RMSE`), color= "Kensington and Chelsea"), stat="identity") +
  geom_line(aes(x= model, y= as.numeric(`Kensington and Chelsea RMSE`), color= "Kensington and Chelsea"), stat="identity") +
  labs(y = "RMSE of log predictions", x = "Model", title= "RMSE values of log-prediction, after cross-validation, training sample") +
  theme(legend.position=c(1, 1), 
        legend.title=element_blank(),
        legend.background = element_rect(fill="white", size=0.5, linetype="solid"))
```
However, these are RMSE-s of the log_price predictions - while we are after predicting the actual prices. Let's fix this.  

As the first step, we'll make predictions on the trainings set with all our models. Reference for how to calculate these predictions predictions can be found here: https://www.r-bloggers.com/forecasting-from-log-linear-regressions/.  
  
  
Implementations is via the below code, which calculates the log, uncorrected level, and corrected level predictions for a given model on a given dataset:
```{r}
predictions_calc <- function(model, dt) {
  log_pred <- predict(model, dt)
  n <- dt[,.N]
  k <- length(predictors(model))
  s2 <- sum((log_pred - dt$log_price)^2) / (n-k)
  
  log_pred <- data.table(log_pred)
  pred_uncorrected <- data.table(exp(log_pred))
  pred <- data.table(exp(log_pred + s2 / 2))
  
  dt_pred <- cbind(log_pred, pred_uncorrected, pred)
  setnames(dt_pred, c("Log Prediction", "Uncorrected Prediction", "Prediction"))
  
  return(dt_pred)
}
```


Calculating RMSE based on the level predictions (both the uncorrected and the corrected):
```{r}
model_1_london_pred <- predictions_calc(model_1_london, london_train)

model_1_london_rmse_uncorr_train <- RMSE(model_1_london_pred$`Uncorrected Prediction`, london_train$price)
model_1_london_rmse_train <- RMSE(model_1_london_pred$`Prediction`, london_train$price)
```

To save ink-space, the code for the other 7 model is not shown, but the exact same logic is applied to those as well to calculate RMSEs.

```{r echo= FALSE}
#1
model_1_kensington_chelsea_pred <- predictions_calc(model_1_kensington_chelsea, kensington_chelsea_train)
model_1_kensington_chelsea_rmse_uncorr_train <- RMSE(model_1_kensington_chelsea_pred$`Uncorrected Prediction`, kensington_chelsea_train$price)
model_1_kensington_chelsea_rmse_train <- RMSE(model_1_kensington_chelsea_pred$`Prediction`, kensington_chelsea_train$price)

#2
model_2_london_pred <- predictions_calc(model_2_london, london_train)
model_2_london_rmse_uncorr_train <- RMSE(model_2_london_pred$`Uncorrected Prediction`, london_train$price)
model_2_london_rmse_train <- RMSE(model_2_london_pred$`Prediction`, london_train$price)

model_2_kensington_chelsea_pred <- predictions_calc(model_2_kensington_chelsea, kensington_chelsea_train)
model_2_kensington_chelsea_rmse_uncorr_train <- RMSE(model_2_kensington_chelsea_pred$`Uncorrected Prediction`, kensington_chelsea_train$price)
model_2_kensington_chelsea_rmse_train <- RMSE(model_2_kensington_chelsea_pred$`Prediction`, kensington_chelsea_train$price)

#3
model_3_london_pred <- predictions_calc(model_3_london, london_train)
model_3_london_rmse_uncorr_train <- RMSE(model_3_london_pred$`Uncorrected Prediction`, london_train$price)
model_3_london_rmse_train <- RMSE(model_3_london_pred$`Prediction`, london_train$price)

model_3_kensington_chelsea_pred <- predictions_calc(model_3_kensington_chelsea, kensington_chelsea_train)
model_3_kensington_chelsea_rmse_uncorr_train <- RMSE(model_3_kensington_chelsea_pred$`Uncorrected Prediction`, kensington_chelsea_train$price)
model_3_kensington_chelsea_rmse_train <- RMSE(model_3_kensington_chelsea_pred$`Prediction`, kensington_chelsea_train$price)

#4
model_4_london_pred <- predictions_calc(model_4_london, london_train)
model_4_london_rmse_uncorr_train <- RMSE(model_4_london_pred$`Uncorrected Prediction`, london_train$price)
model_4_london_rmse_train <- RMSE(model_4_london_pred$`Prediction`, london_train$price)

model_4_kensington_chelsea_pred <- predictions_calc(model_4_kensington_chelsea, kensington_chelsea_train)
model_4_kensington_chelsea_rmse_uncorr_train <- RMSE(model_4_kensington_chelsea_pred$`Uncorrected Prediction`, kensington_chelsea_train$price)
model_4_kensington_chelsea_rmse_train <- RMSE(model_4_kensington_chelsea_pred$`Prediction`, kensington_chelsea_train$price)
```


Let's visualize the results to facilitate our selection of the best model:
```{r, echo=FALSE, fig.width= 15}
m_rmse_uncorr <- matrix(c(1, model_1_london_rmse_uncorr_train, model_1_kensington_chelsea_rmse_uncorr_train, 
                   2, model_2_london_rmse_uncorr_train, model_2_kensington_chelsea_rmse_uncorr_train, 
                   3, model_3_london_rmse_uncorr_train, model_3_kensington_chelsea_rmse_uncorr_train, 
                   4, model_4_london_rmse_uncorr_train, model_4_kensington_chelsea_rmse_uncorr_train), 
                 nrow= 4, byrow= TRUE)


colnames(m_rmse_uncorr) <- c('model', 'London RMSE', "Kensington and Chelsea RMSE")
m_rmse_uncorr.table <- data.table(m_rmse_uncorr)
p1 <- ggplot(data= m_rmse_uncorr.table) +
  geom_point(aes(x= model, y= as.numeric(`London RMSE`), color= "London"), stat="identity", size= 3) +
  geom_line(aes(x= model, y= as.numeric(`London RMSE`), color= "London"), stat="identity", size= 1.5) +
  geom_label(aes(x= model, y= as.numeric(`London RMSE`), label= round(as.numeric(`London RMSE`),2)), hjust= 0, vjust= 0) +
  geom_point(aes(x= model, y= as.numeric(`Kensington and Chelsea RMSE`), color= "Kensington and Chelsea"), stat="identity", size= 3) +
  geom_line(aes(x= model, y= as.numeric(`Kensington and Chelsea RMSE`), color= "Kensington and Chelsea"), stat="identity", size= 1.5) +
  geom_label(aes(x= model, y= as.numeric(`Kensington and Chelsea RMSE`), label= round(as.numeric(`Kensington and Chelsea RMSE`),2)), hjust= 0, vjust= 0) +
  labs(y = "RMSE of uncorrected predictions", x = "Model", title= "RMSE values of uncorrected level-prediction,\nafter cross-validation, training sample") +
  theme(legend.position="none")

#corrected rmse
m_rmse <- matrix(c(1, model_1_london_rmse_train, model_1_kensington_chelsea_rmse_train, 
                   2, model_2_london_rmse_train, model_2_kensington_chelsea_rmse_train, 
                   3, model_3_london_rmse_train, model_3_kensington_chelsea_rmse_train, 
                   4, model_4_london_rmse_train, model_4_kensington_chelsea_rmse_train), 
                 nrow= 4, byrow= TRUE)


colnames(m_rmse) <- c('model', 'London RMSE', "Kensington and Chelsea RMSE")
m_rmse.table <- data.table(m_rmse)
p2 <- ggplot(data= m_rmse.table) +
  geom_point(aes(x= model, y= as.numeric(`London RMSE`), color= "London"), stat="identity", size= 3) +
  geom_line(aes(x= model, y= as.numeric(`London RMSE`), color= "London"), stat="identity", size= 1.5) +
  geom_label(aes(x= model, y= as.numeric(`London RMSE`), label= round(as.numeric(`London RMSE`),2)), hjust= 0, vjust= 0) +
  geom_point(aes(x= model, y= as.numeric(`Kensington and Chelsea RMSE`), color= "Kensington and Chelsea"), stat="identity", size= 3) +
  geom_line(aes(x= model, y= as.numeric(`Kensington and Chelsea RMSE`), color= "Kensington and Chelsea"), stat="identity", size= 1.5) +
  geom_label(aes(x= model, y= as.numeric(`Kensington and Chelsea RMSE`), label= round(as.numeric(`Kensington and Chelsea RMSE`),2)), hjust= 0, vjust= 0) +
  labs(y = "RMSE of predictions", x = "Model", title= "RMSE values of level-prediction,\nafter cross-validation, training sample") +
  theme(legend.position=c(1, 1), 
        legend.title=element_blank(),
        legend.background = element_rect(fill="white", size=0.5, linetype="solid"))

grid.arrange(p1, p2, ncol = 2)
```

So far it looks like that the predictions made by the most complex model, tuned with LASSO regularization yields the best results. Given the only thing we are after is good prediction, I am picking this model as the best performing one, based on RMSE criteria as a metric of performance.

The only step remaining regarding validation of results is checking if the predictions look well-calibrated - meaning that broken to smaller groups, the avg. predicted values should align with average actuals.  
  
Let's see if this is true on the below plots:
```{r, echo= FALSE, fig.align='center', fig.width= 15, fig.height= 4}
actual_vs_predicted <- data.table(actual = london_train$price,
                                  predicted = model_4_london_pred$`Prediction`)

actual_vs_predicted[, category := cut(predicted,
                                    seq(0, 150, 25),
                                    include.lowest = TRUE)]
calibration <- actual_vs_predicted[, .(mean_actual = mean(actual),
                                       mean_predicted = mean(predicted),
                                       num_obs = .N),
                                   keyby = .(category)]
p1<- ggplot(calibration,
       aes(x = mean_actual, y = mean_predicted, size = num_obs)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
  ylim(0, 250) + xlim(0, 250) +
  labs(title = "Calibration of predictions - Full London", subtitle= "Comparison of mean predicted and actual prices by groups")

#
actual_vs_predicted <- data.table(actual = kensington_chelsea_train$price,
                                  predicted = model_4_kensington_chelsea_pred$`Prediction`)

actual_vs_predicted[, category := cut(predicted,
                                    seq(0, 150, 25),
                                    include.lowest = TRUE)]
calibration <- actual_vs_predicted[, .(mean_actual = mean(actual),
                                       mean_predicted = mean(predicted),
                                       num_obs = .N),
                                   keyby = .(category)]
p2 <- ggplot(calibration,
       aes(x = mean_actual, y = mean_predicted, size = num_obs)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
  ylim(0, 250) + xlim(0, 250) +
  labs(title = "Calibration of predictions - Kensington and Chelsea", subtitle= "Comparison of mean predicted and actual prices by groups")

grid.arrange(p1, p2, ncol = 2)
```

Apart from a small difference on the last group, our model looks nicely calibrated.  
For this group, if we think about the price distribution, we can recall that there are some values lying long apart from others - we have a right-skewed distribution of prices.  
For such prices with not many observations, fitting a linear model would hurt general fit for more dense price ranges - hence it can be concluded that the highest-prices' group being a bit off is rather a feature of the model, than a bug.


#### Evaluating the best model  
  
  
After evaluating the different models based on different criteria, and selecting the best ones, let's see how they perform on the held-out test set:
```{r, echo= FALSE}
model_4_london_pred_test <- predictions_calc(model_4_london, london_test)
model_4_london_rmse_uncorr_test <- RMSE(model_4_london_pred_test$`Uncorrected Prediction`, london_test$price)
model_4_london_rmse_test <- RMSE(model_4_london_pred_test$`Prediction`, london_test$price)

model_4_kensington_chelsea_pred_test <- predictions_calc(model_4_kensington_chelsea, kensington_chelsea_test)
model_4_kensington_chelsea_rmse_uncorr_test <- RMSE(model_4_kensington_chelsea_pred_test$`Uncorrected Prediction`, kensington_chelsea_test$price)
model_4_kensington_chelsea_rmse_test <- RMSE(model_4_kensington_chelsea_pred_test$`Prediction`, kensington_chelsea_test$price)

m_rmse_test <- matrix(c("Uncorrected", model_4_london_rmse_uncorr_test, model_4_kensington_chelsea_rmse_uncorr_test, 
                   "corrected", model_4_london_rmse_test, model_4_kensington_chelsea_rmse_test), 
                 nrow= 2, byrow= TRUE)


colnames(m_rmse_test) <- c('prediction', 'London RMSE', "Kensington and Chelsea RMSE")
data.table(m_rmse_test)
```
  
  
Given the above RMSE-s are looking somewhat large, I had to think whether the model is actually bad.  
Let's remember the calibration plot - we could already see that really larger prices were off a bit.  
This should be no surprise - it is really hard to predict tail values, which are extreme in value, but happen with a rare frequency.  
At the start I've made a choice of building the model on prices available - in this case, that might not be the best decision if we are really concerned with making the best predictions. (One way of getting around this problem could be building separate models for "normal" and "high prices", which might be prone to completely different dynamics)  
  
We can still check the RMSE value if we only calculate it for limited observations, like below:  

```{r}
x <- data.table(cbind(model_4_london_pred_test$`Prediction`, london_test$price))
setnames(x, c("prediction", "price"))
x <- x[price<500, ]

RMSE(x$prediction,x$price)
```

The difference is significant - so we can conclude that the model is not actually bad at making predictions, it just can't handle the more extreme values, due to it's linear structure.

#### London prices 2018

So far we tested the model in the data we already have. Creating the best model to predict our existing data is obviously not the goal - why would we predict something we already know, after all.  
Here, we turn to the question of external validity - we want to pick the model, which will best predict prices that are not yet known. There are factors that might change between our dataset and what will happen in 2018 (let's just mention one: BrExit is continuing with full throttle), which is not captured by the variables in our model.  
Generally, one thing we can do in these cases is make the model less complex (if we can't capture all impacting variables). Basically, by doing this, we give way to a little bias possibly (we are not making the best predictions we could), in exchange for less variance. Or, with other words, we sacrafice precision, hoping to gain accuracy - a good deal most likely.  
  
Does this problem apply here? Should we chose something other than the most complex model, which had the best prediction power? My answer is a mix of no and a maybe. I'm by no means an expert in the dynamics of London's accomodation market, so I'm not going to take a side here. But if there is reason to believe that something will be fundamentally different in 2018, then go with a simpler model (maybe model 3 or even 2). If the outlook is not changing based on expert judgment, then I think all the work done to avoid overfitting (CV, LASSO) should give us enough confidence to apply the most complex model.  

#### Budapest prices 2018
  
Here, we take a step even further - we don't just move in the future in terms of the time dimension, but also we want to predict for a different location (one reason I left out the borough as a predictor, is that at least theoretically the model could work for Budapest this way).  
  
So, what change? Well, a lot - what differentiates pricing in London could be totally different from what factors are important in Budapest. Just think about it - Budapest is a cheap location for most Westerners, and they come in all different shapes and sizes, but most likely come either for the sights, or for the nightlife. Many our young people looking to have fun at a reasonable price. Now think about London, one of the World's most expensive cities, with a much different city size, infrastructure, etc.  

This might be the exact case when less is more, addition by subtraction. If we apply a complex model, then it will capture the pricing dynamics likely in detail - which is not a good thing if the dynamics are different in London and Budapest. I'd choose a much less complex model (either the second or maybe the third) for this purpose - one, that I built based on my (limited) understanding of what could decide prices in general, factors that are likely the same all around the World.  
  
What I'm hoping to achieve with this is simple: rather than creating a false illusion of precision, let's accept that we won't be able to use this model to get very exact prices, and be satisfied with just separating general price levels from each other.
