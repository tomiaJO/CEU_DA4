---
title: "DA4 -  Assignment 1"
author: "Tamas Koncz"
date: '2018-02-11'
output:
  html_document:
    df_print: paged
  html_notebook:
    df_print: paged
  pdf_document: default
---

```{r setup, message=FALSE, include=FALSE}
library(data.table)

library(ggplot2)
library(gridExtra)
library(scales)
library(reshape)

library(caret)
library(glmnet)
library(ROCR)

library(skimr)
library(knitr)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)

theme_set(theme_minimal())   # globally set ggplot theme

set.seed(93)
RMSE <- function(x, true_x) sqrt(mean((x - true_x)^2))
ERR_SQ <- function(x, true_x) (x - true_x)^2
```


```{r, include= FALSE}
rm(list=ls())
data <- fread("./../Data/DA4/bisnode_all.csv",
              stringsAsFactors = FALSE)
```

```{r, echo= FALSE}
fun_count_na <- function(dt) {
  missing_values <- as.data.table(t(dt[, lapply(.SD, function(x) sum(is.na(x))), .SDcols = names(dt)]),
                                  keep.rownames = TRUE)
  setnames(missing_values, c("variable", "NA.Count"))
  
  return(missing_values[order(-NA.Count)][NA.Count>0])
}
```

### Understanding the dataset  

After simply reading in the dataset, first step is to understand what we are working with.
A key component of data quality is completness, which here is defined as having as little not-known values as possible for our variables.

A glimpse into the missing values, order by most NAs, top 10 below:
```{r, echo = FALSE}
fun_count_na(data)[1:10]
```
  
It's clear that the dataset is not "NA-sparse" by any means.
My approach to address this here is the following:   
    - for some key variables, having NAs is not toleratable. As we are going to predict financial performance, a good example of key variables are liabilites or assets. If such variables are missing, the whole observation will be dropped.  
    - for other variables, which are likely less important, if there are too many missing values, I'm simplying not using the variable for prediction. An example could be birth_year (CEO) - dropping these observations could be costly in terms of limiting our working sample too much (for the benefit we could gain), so rather I chose not to include these type of variables.  
    
  
An example of removing some of the observations that are missing information in high-significance columns:  
```{r}
data <- data[!is.na(curr_assets) | !is.na(curr_liab)]
data <- data[!(is.na(ind) | ind == "")]
data <- data[!(is.na(region_m) | region_m == "")]
data <- data[!is.na(founded_year)]
```

A short example of removing some of the variables that are likely not useful, given the very large number of NAs:
```{r}
data[, c("D", 
         "COGS", 
         "finished_prod", 
         "exit_year",
         "wages") := NULL]
```

  
The next step to take is to compile the dataset in a format that will be usable for predictions (every predictor in its on row, every observational unit is in its on column).  
  
  
Generally, multiple years of data are used for prediction. The "current year" is when the default happens - and prediction is based on the financials and some other variables in the preceding two years.   
There is a purposeful limitation here - any company that hasn't been active for at least two years will be dropped. This is a significant constraint, but one with a business-minded driver ones. Hence, in my view, their default prediction should be based on separate models and likely a lot more expert judgement.  
Technically, this was a trade-off of having a good-model for a limited scope versus a likely less-good (as the available predictors would be more shallow) model for a larger scope. I chose the first option.  


```{r, echo= FALSE}
data <- data[, .(comp_id, 
                  year= year, 
                  curr_liab= curr_liab,
                  curr_assets= curr_assets,
                  curr_sales= sales,
                  founded_year = founded_year,
                  industry = ind,
                  region = region_m)]

data_prev <- data[year < 2016,.(comp_id, 
                     year= year + 1, 
                     prev_liab= curr_liab,
                     prev_assets= curr_assets,
                     prev_sales= curr_sales,
                     prev_founded_year = founded_year,
                     prev_industry = industry,
                     prev_region = region)]

#only financials
data_prev2 <- data[year < 2015,.(comp_id, 
                     year= year + 2, 
                     prev_2_liab= curr_liab,
                     prev_2_assets= curr_assets,
                     prev_2_sales= curr_sales)]

data_prev <- merge(x = data_prev, y = data_prev2, by = c("comp_id", "year"), all.x = FALSE, all.y= FALSE) #filters down population. #TODO explain consequences

dt <- merge(x = data, y = data_prev, by = c("comp_id", "year"), all.x = FALSE, all.y= TRUE)

#only use years where the full dataset is available generally
dt <- dt[year != 2011 &
          year != 2012 &
          year != 2016]

rm(data)
rm(data_prev)
rm(data_prev2)
```
  
  
The final dataset I am working with is based of companies and defaults happening between 2013 - 2015.  


```{r, include= FALSE}
fun_replace_na <- function(x) {
  x <- ifelse(is.na(x), 0, x)
  return(x)
}
```

  
The last important step of data cleansing was to:  
    - recode the industry variable to have only two levels (from previously three), by merging the small "Manufacturing" cohort into the larger one
    - some metrics that were left as NAs (see table below) were recoded as 0, as upon inspecting them I made the judgment that these are likely rather true 0-s than missing data points (so observations weren't missing just at random)  
    
    
```{r, echo= FALSE}

fun_count_na(dt)

#fix founded_year
dt[, is_new := ifelse(!is.na(founded_year) & is.na(prev_founded_year), 1, 0)]
dt[, is_dropped := ifelse(is.na(founded_year) & !is.na(prev_founded_year), 1, 0)]

dt[, founded_year := ifelse(is.na(founded_year), prev_founded_year, founded_year)]
dt[, prev_founded_year := NULL]

dt[, industry := ifelse(is.na(industry), prev_industry, industry)]
dt[, prev_industry := NULL]
dt[industry %like% '*Manuf*', industry := "Manufacturing"] # recode & merge due to small number of observations
dt[industry %like% '*hotel*', industry := "Leisure"]

dt[, region := ifelse(is.na(region), prev_region, region)]
dt[, prev_region := NULL]
dt[, region := factor(region, levels= c("West", "Central", "East"))]

cols = names(dt)
dt[, (cols) := lapply(.SD, fun_replace_na), .SDcols = cols]
```

Now it's time to create an indicator for default.  
  
[Definition for default](https://www.investopedia.com/terms/d/default2.asp): "... debtor is unable to meet the legal obligation of debt repayment...". Hence, the first requirement is to have some obligations to begin with.  

The definition of default for this exercise will follow the along this logic: looking at companies who had any liabilities during a year, but their performance deteoriated significantly the next, measured by sales dropping to 0.
This definition is likely to be punitive, and not exactly relating to companies who have defaulted, but there are trade-offs in every decision in the lack-of any clear description for defaulted parties in the data.
Limitation of this approach are:
    - Sales is not a good measure for default. What we would actually be interested in is companies filing for default on court, but given that's not available, we use sales dropping to zero as an indicatory dummy for businesses "closing down".  
    However considering i.e. a project firm, that was created for a set of time purposefully - if these firms are not operating any longer those are not real defaults.
    - We don't know what happened to firms that suddenly "exit" the dataset. As a conservative approach (in terms of defining default scenarios as wide as possible), if a firm is no longer in the data, but had liabilities as of the last records, that will be considered a default.

Code to create the target variable, is_defaulted:
```{r}
dt[, is_defaulted := ifelse((curr_sales == 0 & prev_sales > 0) &
                            (curr_liab > 0 | (prev_liab > 0 & is_dropped == 1))
                            , 1, 0)]
```
  
  
Summary by year (for reference, see: [here](https://www.ksh.hu/docs/hun/xftp/gyor/gaz/gaz1612.pdf) on p4 there is a table of "defaults" by industry):
  
```{r, fig.align='center', echo= FALSE}
default_viz <- dt[, .(count= .N,
       default_count= sum(is_defaulted),
       default_pct= sum(is_defaulted) / .N * 100,
       drop_count= sum(is_dropped),
       new_count= sum(is_new)),
   keyby = .(industry, region, year)]

max_default_pct <- default_viz[, max(default_pct)]
max_count <- default_viz[, max(count)]

scaleFUN <- function(x) sprintf("%.2f", x)

ggplot(data= default_viz) + 
  geom_bar(aes(x= year, y= count, fill = "# of observations"), stat= "identity") +
  geom_point(aes(x= year, y= default_pct * (max_count / max_default_pct), color = "% of defaults"), stat= "identity") +
  geom_line(aes(x= year, y= default_pct * (max_count / max_default_pct), color = "% of defaults"), stat= "identity") +
  facet_grid(industry ~ region) +
  scale_y_continuous(sec.axis = sec_axis(~./(max_count/max_default_pct), name = "% of defaults", labels=scaleFUN), 
                     labels = scales::comma) + 
  scale_color_manual(name = "Legend", values = c("% of defaults" = "darkblue")) + 
  scale_fill_manual(name = "Legend", values = c("# of observations" = "tomato")) + 
  labs(y = "# of observations", x = "Year", 
       title = "Visualizing all companies, with default rates", subtitle = "Breakdown by Region and Industry")

rm(default_viz)
rm(max_default_pct)
rm(max_count)
```
  
  
#### Visual data exploration

Some of the variables (Assets, Liabilities, and Sales) are close to being log-normally distributed:  

```{r, echo = FALSE, fig.align='center', fig.width=6, fig.height=4}
temp <- dt[, .(industry, 
               region, 
               log_liab= log(curr_liab), 
               log_assets= log(curr_assets),
               log_sales= log(curr_sales))]
temp2 <- melt(temp, id.vars = c("industry", "region"))

ggplot(data= temp2, aes(x = value, fill = variable)) + 
  geom_density(alpha=0.25) +
  facet_grid(industry ~ region) + 
  labs(title = "Density diagrams of Assets, Liabilities and Sales", 
       subtitle = "Breakdown by region & industry")

rm(temp)
rm(temp2)
```

These will be Log-transformated, with a little correction term (for log(0) = -Inf):
```{r}
dt[, prev_liab := ifelse(prev_liab>0, prev_liab, 0.1)]
dt[,log_liab := log(prev_liab)]
dt[, prev_2_liab := ifelse(prev_2_liab>0, prev_2_liab, 0.1)]
dt[,log_liab_2 := log(prev_2_liab)]

dt[, prev_assets := ifelse(prev_assets>0, prev_assets, 0.1)]
dt[,log_asset := log(prev_assets)]
dt[, prev_2_assets := ifelse(prev_2_assets>0, prev_2_assets, 0.1)]
dt[,log_asset_2 := log(prev_2_assets)]

dt[, prev_sales := ifelse(prev_sales>0, prev_sales, 0.1)]
dt[,log_sales := log(prev_sales)]
dt[, prev_2_sales := ifelse(prev_2_sales>0, prev_2_sales, 0.1)]
dt[,log_sales_2 := log(prev_2_sales)]
```

Dropping variables which are not appropriate for prediction, given their availability might be only _after-the-fact_ of the default.

```{r}
dt[, curr_liab := NULL]
dt[, curr_assets := NULL]
dt[, curr_sales := NULL]

dt[, is_new := NULL]
dt[, is_dropped := NULL]

dt[, comp_id := NULL]
```
  
Company age:  

```{r, echo= FALSE}
dt[, comp_age := year - founded_year]
```


```{r, echo= FALSE}
data_by_comp_age <- dt[ ,.(default_rate = mean(is_defaulted), num_obs = .N), keyby = .(age_category = cut(comp_age, breaks = c(0, 1, 2, 3, 4, 5, 10, 15, 20, 30, Inf), include.lowest = TRUE))]

p1 <- ggplot(data = data_by_comp_age,
       aes(x = age_category, y = default_rate, size = num_obs)) +
  geom_point() +
  ylim(0, NA) +
  labs(y = "Default frequencies", x = "Age Category", title = "Relationship between company age and default rates", subtitle = "(Note: Age buckets are not same length)") +
  theme(plot.subtitle=element_text(face="italic"))
  
```

```{r, echo= FALSE}
#liab
data_by_liab <- dt[ ,.(default_rate = mean(is_defaulted), num_obs = .N), keyby = .(liab_category = cut(log(prev_liab), breaks = c(-1, 0, 1, 2, 3, 5, 8, 9, 10, 11, 13, 20, Inf), include.lowest = TRUE))]

p2 <- ggplot(data = data_by_liab,
       aes(x = liab_category, y = default_rate, size = num_obs)) +
  geom_point() +
  ylim(0, NA) +
  labs(y = "Default frequencies", x = "Liabilities Category", title = "Relationship between log(Liabilities) and default rates", subtitle = "(Note: Buckets are not same length)") +
  theme(plot.subtitle=element_text(face="italic"))
```

```{r, echo= FALSE}
#assets
data_by_asset <- dt[ ,.(default_rate = mean(is_defaulted), num_obs = .N), keyby = .(asset_category = cut(log(prev_assets), breaks = c(-1, 0, 1, 2, 3, 5, 8, 9, 10, 11, 13, 20, Inf), include.lowest = TRUE))]

p3 <- ggplot(data = data_by_asset,
       aes(x = asset_category, y = default_rate, size = num_obs)) +
  geom_point() +
  ylim(0, NA) +
  labs(y = "Default frequencies", x = "Assets Category", title = "Relationship between log(Assets) and default rates", subtitle = "(Note: Buckets are not same length)") +
  theme(plot.subtitle=element_text(face="italic"))
```

```{r, echo = FALSE, fig.align='center', fig.width=15, fig.height=4}
grid.arrange(p1, p2, p3, ncol = 3)
rm(p1)
rm(p2)
rm(p3)
```

Different relationship patterns between default frequencies and the predictors can be observed:  
    1. for Age, the pattern is relatively linear - for simplicity reasons, no extra term will be created to help fit non-linearities in the logistic regression models  
    2. for Liabilities, the dynamics are more complicated. This is likely due to the fact that while more liabilities can be considered a burden, we can observe that larger companies (who tend to hold more liabilities as well in absolute terms) are less likely to suddenly go bust  
    3. For Assets, there is a clear negative relationship between default frequencies and assets, however it is not linear  

Let's address this with the inclusion of some polynomial terms:  

```{r}
#prev log(liab)
dt[, log_liab_sq := log_liab^2]
dt[, log_liab_cub := log_liab^2]

#t-2 log(liab)
dt[, log_liab_2_sq := log_liab_2^2]
dt[, log_liab_2_cub := log_liab_2^2]

#prev log(assets)
dt[, log_asset_sq := log_asset^2]

#t-2 log(assets)
dt[, log_asset_2_sq := log_asset_2^2]
```

```{r, echo = FALSE}
dt[, c("prev_liab", "prev_assets", "prev_sales",
       "prev_2_liab", "prev_2_assets", "prev_2_sales") := NULL]
```
  
  
Calculate a couple of YoY change ratios:  

```{r}
dt[, liab_chg := log_liab - log_liab_2]
dt[, liab_chg := ifelse(!is.na(liab_chg), liab_chg, 1)]
dt[, asset_chg := log_asset - log_asset_2]
dt[, asset_chg := ifelse(!is.na(asset_chg), asset_chg, 1)]
dt[, sales_chg := log_sales - log_sales_2]
dt[, sales_chg := ifelse(!is.na(sales_chg), sales_chg, 1)]
```
    
```{r, echo= FALSE}
# liab_chg
data_by_liab_chg <- dt[ ,.(default_rate = mean(is_defaulted), num_obs = .N), keyby = .(liab_chg_category = cut(liab_chg, breaks = 10))]

p1 <- ggplot(data = data_by_liab_chg,
       aes(x = liab_chg_category, y = default_rate, size = num_obs)) +
  geom_point() +
  ylim(0, NA) +
  labs(y = "Default frequencies", x = "Liab. Growth Category", title = "Change in Log(Liabilities) and default rates", subtitle = "(Note: Buckets are not same length)") +
  theme(plot.subtitle=element_text(face="italic"))

# asset_chg
data_by_asset_chg <- dt[ ,.(default_rate = mean(is_defaulted), num_obs = .N), keyby = .(asset_chg_category = cut(asset_chg, breaks = 10))]

p2 <- ggplot(data = data_by_asset_chg,
       aes(x = asset_chg_category, y = default_rate, size = num_obs)) +
  geom_point() +
  ylim(0, NA) +
  labs(y = "Default frequencies", x = "Assets Growth Category", title = "Change in Log(Assets) and default rates", subtitle = "(Note: Buckets are not same length)") +
  theme(plot.subtitle=element_text(face="italic"))

#sales_chg
data_by_sales_chg <- dt[ ,.(default_rate = mean(is_defaulted), num_obs = .N), keyby = .(sales_chg_category = cut(sales_chg, breaks = 10))]

p3 <- ggplot(data = data_by_sales_chg,
       aes(x = sales_chg_category, y = default_rate, size = num_obs)) +
  geom_point() +
  ylim(0, NA) +
  labs(y = "Default frequencies", x = "Sales Growth Category", title = "Change in Log(Sales) and default rates", subtitle = "(Note: Buckets are not same length)") +
  theme(plot.subtitle=element_text(face="italic"))
```

```{r, echo = FALSE, fig.align='center', fig.width=15, fig.height=4}
grid.arrange(p1, p2, p3, ncol = 3)
rm(p1)
rm(p2)
rm(p3)
```

We can see that the relationsip types for these variables are all-over the place. If we wanted to model these and their interactions properly, that could be done with further polynomial terms, dummy variables for some cuts, etc.  
  
However, here the goal is to create the best predictive model, with the great-ease that this being a University project, there is no need to create an explainable model for getting buy-in from stakeholders.  
Hence, we will see the power of some flexible tree-based ML-models, which can model these relationships without much analyst adjustment - and I am not going to create further extra terms.  

    
```{r, echo = FALSE}
dt[, is_defaulted:= ifelse(is_defaulted==1, "Yes", "No")]
dt[, is_defaulted := factor(is_defaulted, levels = c("Yes", "No"))]
```

### Prediction: training models
Note before we jump into prediction: deliberately, I used variables only from the year(s) _before_ the default. This is to make sure that only information was used for prediction which is available aprior.


There will be 3 sub-samples used:  
    1. A training set, 60% of the whole dataset, used to train & tune models via 10-fold CV.  
    2. A test set, 30% of the whole dataset, which is used to compare models and pick the best one for our purpose  
    3. A final performance evaulation set, 10% of the whole data, which is to calculate independent performance metrics (as the previous two sets were used for training and picking the mode, we need an "independent" set to measure real expected performance)  
    
    
```{r}
training_ratio <- 0.6

set.seed(93) #for reproducibility
train_indices <- createDataPartition(y = dt[["is_defaulted"]],
                                     times = 1,
                                     p = training_ratio,
                                     list = FALSE)
data_train <- dt[train_indices, ]
data_test <- dt[-train_indices, ]

split_ratio <- 3/4

set.seed(93) #for reproducibility
perf_indices <- createDataPartition(y = data_test[["is_defaulted"]],
                                     times = 1,
                                     p = split_ratio,
                                     list = FALSE)
data_test <- data_test[perf_indices, ]
data_perf <- data_test[-perf_indices, ]
```
  
  
  
Control function, setting 10-fold CV, and telling the used caret-package that we are predicting binary categorizes (classification setup):  

```{r}
train_control <- trainControl(method = "cv",
                              number = 10,
                              classProbs = TRUE,
                              summaryFunction = twoClassSummary)
```

#### Model 1 - basic logit  
  
The first model trained is a simple logit model, with only a subset of variables used.  
  
(For this model, and all others, caret is set to maximize the AUC)  

```{r}
#base model
set.seed(93)
glm_model <- train(is_defaulted ~ year + industry + region + log_liab + log_liab_2 + log_asset + log_asset_2 + log_sales + log_sales_2,
                   method = "glm",
                   family = "binomial",
                   metric = "ROC",
                   data = data_train,
                   trControl = train_control)

```
  
In total 4 models are being created - after each are done, there performance will be compared.  
  
```{r, echo = FALSE}
glm_prediction_probs <- predict.train(glm_model, newdata = data_test, type= "prob")
test_truth <- data_test[["is_defaulted"]]
```

```{r, echo= FALSE}
fun_calc_auc <- function(prediction_probs, truth) {
  
  rocr_prediction <- prediction(prediction_probs$Yes, truth)
  AUC <- performance(rocr_prediction, "auc")@y.values[[1]]
  
  return(AUC)
}
```

```{r, echo = FALSE}
fun_roc_create <- function(prediction_probs, truth, AUC= NULL, model= NULL) {

  min_prob <- min(prediction_probs$Yes)
  max_prob <- max(prediction_probs$Yes)
  
  thresholds <- seq(0, max_prob, by = (max_prob - min_prob) / 100)
  
  true_positive_rates <- rep(0, length(thresholds))
  false_positive_rates <- rep(0, length(thresholds))
  
  for (ix in 1:length(thresholds)) {
    thr <- thresholds[ix]
    test_prediction <- ifelse(prediction_probs$Yes > thr, "Yes", "No")
    test_prediction <- factor(test_prediction, levels = c("Yes", "No"))
    cm <- as.matrix(confusionMatrix(test_prediction, truth))
    true_positive_rates[ix] <- cm[1, 1] / (cm[1, 1] + cm[2, 1])
    false_positive_rates[ix] <- cm[1, 2] / (cm[1, 2] + cm[2, 2])
  }
  
  manual_roc <- data.table("threshold" = thresholds,
                           "true_positive_rate" = true_positive_rates,
                           "false_positive_rate" = false_positive_rates)
  
  s_title <- ifelse(is.null(model), "", paste("ROC curve for Model:", model, sep = " "))
  s_subtitle <- ifelse(is.null(AUC), "", paste("AUC =", round(AUC, 2), sep = " "))
  
  ggplot(data = manual_roc, 
         aes(x = false_positive_rate, y = true_positive_rate, color = threshold)) +
    geom_point() +
    geom_abline(intercept = 0, slope = 1,  linetype = "dotted", col = "black") +
    labs(title = s_title, subtitle = s_subtitle)
}
```

```{r, echo = FALSE}
fun_calibration_create <- function(prediction_probs, truth, model= NULL) {
  
  truth_numeric <- ifelse(truth == "Yes", 1, 0)
  score <- prediction_probs$Yes
  
  actual_vs_predicted <- data.table(actual = truth_numeric,
                                    predicted = score)
  
  actual_vs_predicted[, score_category := cut(predicted,
                                      seq(0, 0.6, 0.05),
                                      include.lowest = TRUE)]
  
  calibration <- actual_vs_predicted[, .(mean_actual = mean(actual),
                                         mean_predicted = mean(predicted),
                                         num_obs = .N),
                                     keyby = .(score_category)]
  
  s_title <- ifelse(is.null(model), "", paste("Calibration plot for Model:", model, sep = " "))
  
  ggplot(calibration,
         aes(x = mean_actual, y = mean_predicted, size = num_obs)) +
    geom_point() +
    geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
    ylim(0, 1) + xlim(0, 1) +
    labs(title = s_title)
}
```

```{r, echo= FALSE}
glm_AUC <- fun_calc_auc(glm_prediction_probs, test_truth)
glm_ROC <- fun_roc_create(glm_prediction_probs, test_truth, glm_AUC, "glm")
glm_calibration <- fun_calibration_create(glm_prediction_probs, test_truth, "glm")
```
  
  
#### Model 2 - regularized logit  
  
The second model to be created is also a Logit-model.  
However, compared to the previous example, we'll let the machine do some of our work - there is no variable selection by analyst judgment.  
Rather, we are using LASSO- and Ridge regularization, and let the lambda parameter be optimized by glmnet's grid-search algorithm (below range for lambda search was pre-selected based on some trial runs).  


```{r}
tune_grid <- expand.grid("alpha" = c(0, 1),
                         "lambda" = seq(0.0001, 0.0002, 0.00001))

set.seed(93)
glmnet_model <- train(is_defaulted ~ .,
                   method = "glmnet",
                   family = "binomial",
                   metric = "ROC",
                   data = data_train,
                   trControl = train_control,
                   tuneGrid = tune_grid,
                   preProcess = c("center", "scale"))

```

```{r, echo = FALSE}
glmnet_prediction_probs <- predict.train(glmnet_model, newdata = data_test, type= "prob")
```

```{r, echo= FALSE}
glmnet_AUC <- fun_calc_auc(glmnet_prediction_probs, test_truth)
glmnet_ROC <- fun_roc_create(glmnet_prediction_probs, test_truth, glmnet_AUC, "glmnet")
glmnet_calibration <- fun_calibration_create(glmnet_prediction_probs, test_truth, "glmnet")
```

#### Model 3 - single classification tree  
  
The third method is a single tree, which has a nice advantage compared to previous methods, as it is able to pick-up non-linearities without much human intervention, simply by creating multiple splitting-nodes of the same variable (similarly it goes for interactions between variables).  
The disadvantages of a single tree will be addressed by the next model.  

```{r}
set.seed(93)
rpart_model <- train(is_defaulted ~ .,
                   data = data_train,
                   method = "rpart",
                   tuneLength = 20,
                   metric = "ROC",
                   trControl = train_control)

```

```{r, echo = FALSE}
rpart_prediction_probs <- predict.train(rpart_model, newdata = data_test, type= "prob")
```

```{r, echo= FALSE}
rpart_AUC <- fun_calc_auc(rpart_prediction_probs, test_truth)
rpart_ROC <- fun_roc_create(rpart_prediction_probs, test_truth, rpart_AUC, "rpart")
rpart_calibration <- fun_calibration_create(rpart_prediction_probs, test_truth, "rpart")
```

#### Model 4 - bagged trees  

The last method being run utilizies bagging - bootstrap aggregation.  
In this example, multiple trees are being fit on equal-sized, bootsrap selected sub-samples, and the final prediction is the aggregate of all these trees' separate prediction.   
This method helps correct for the "locality" problem of using a single tree.  
(Note: I wanted to train a random forest as well, however it was taking significant runtime, while not producing any superior results)

```{r}
set.seed(93)
treebag_model <- train(is_defaulted ~ .,
                   data = data_train,
                   method = "treebag",
                   metric="ROC",
                   trControl = train_control)

```

```{r, echo= FALSE}
treebag_prediction_probs <- predict.train(treebag_model, newdata = data_test, type= "prob")
```

```{r, echo= FALSE}
treebag_AUC <- fun_calc_auc(treebag_prediction_probs, test_truth)
treebag_ROC <- fun_roc_create(treebag_prediction_probs, test_truth, treebag_AUC, "treebag")
treebag_calibration <- fun_calibration_create(treebag_prediction_probs, test_truth, "treebag")
```
  
### Model selection based on performance  
  
```{r, fig.height=15, fig.width=15}
grid.arrange(glm_ROC, glm_calibration,
             glmnet_ROC, glmnet_calibration,
             rpart_ROC, rpart_calibration,
             treebag_ROC, treebag_calibration,
             ncol = 2)
```
  
The primary metric for selecting the best model is Area-Under-the-(ROC)Curve, AUC.  
Also, by plotting the ROC curve we can better understand the trade-offs we have to make based on the models performance.  

In the next section I will elaborate more on these trade-offs. For now, let's see what model should we chose based on performance.  
  
All models performed relatively nicely (at least compared to my initial expectations).  
Performance, measured by AUC, increased together with model complexity - on the test set! So it could be argued that model tuning was done well, overfitting could be avoided even with the more flexible methods (having a not-so-small sample size certainly helped).  
  
ROC curve show that even though there is (as usual) a trade-off in false positive rate and true positive rate when choosing cut-off probabilities, we should be able to make a good decision (something I'll keep for later, with the final model).  

Let's discuss calibrations as well: the picture is more complex here. No model produces well-calibrated probabilities across the board. The large observation groups were more-or-less fitted nicely, however for smaller groups (which could be still significant for our purpose) the results vary a lot.  

This is what led to the decision on the best model as well: while other methods were not consistent, bagging consistently overpredicted the further-from-0 probabilities.  
Is this a good think? In  my view, predicting default probabilities should be a conservative venture.  
  
If I'm a lender, I'm certainly missing out on revenue opportunities if I'm overly risk-averse. However, our loss function should not be symmetrical, neither linear (in terms of expected loss, which is ~default prob. * exposure) in this case - larger losses are almost certainly exponentially more painful than smaller ones, while by virtue of the business of lending (if we predict probabilities for that purpose), gains are limited.  

Hence, my choice for the best model is model #4, tree bagging. Measured by AUC, it's a draw between this one and a single tree.  
However, it has two benefits. One is the aforementioned probability calibration being better fit for the purpose, and the other is robustness - as discussed earlier, a single tree is prone to fall into local optimum. By having a more robust method, I expect we would achieve larger external validty for further use.

### Evaluating performance for the final model  
  
```{r, echo= FALSE}
final_probs <- predict.train(treebag_model, newdata = data_perf, type= "prob")
perf_truth <- data_perf[["is_defaulted"]]
```

```{r, echo= FALSE}
treebag_AUC <- fun_calc_auc(final_probs, perf_truth)
treebag_ROC <- fun_roc_create(final_probs, perf_truth, treebag_AUC, "treebag")
treebag_calibration <- fun_calibration_create(final_probs, perf_truth, "treebag")
```
  
Let's measure performance one more time for the chosen model, for a held-out set specifically for this purpose:  

```{r, echo=FALSE, fig.height=4, fig.width=15}
grid.arrange(treebag_ROC, treebag_calibration, ncol = 2)
```
Results are in line with what we've seen before.  
  
There is a significant step remaining: selecting the cut-off probability for predicting yeses.  
  
Generally, we are trading sensitivity-for-specificity here, which we can optimalize based on the ROC curve.
I want to consider another factor here - as previously mentioned, false negatives are the real cost here. Hence, the resulting cut-off choice is 0.1, and the resulting confusion matrix is below:  
```{r, echo = FALSE}
whole_truth <- dt$is_defaulted
whole_probs <- predict.train(treebag_model, newdata = dt, type= "prob")

whole_pred <- ifelse(whole_probs$Yes > 0.10, "Yes", "No")
whole_pred <- factor(whole_pred, levels = c("Yes", "No"))
m <- as.matrix(confusionMatrix(whole_pred, whole_truth), what = "xtabs")
rownames(m) <- c("Predicted - Yes","Predicted - No")
colnames(m) <- c("Actual - Yes","Actual - No")
m
```

_Note: For this, I re-predicted probabilities for the whole dataset (keep in mind that yes/no-s are very unbalanced, hence I hope to have a more robust choice this way)._

It's true that we do predict a lot of false negatives, but this is a deliberate choice. Actually, we predict ~90% of both defaults and non-defaults correctly, which can be considered a good outcome in my view.  


### Last thoughts 

As the conclusion for this analysis, I'd like to share two things which I consider the main plausible limitations of this study:  


#### Concerns: external validty and implication of sample-limitating decisions  
  
Let's start with sample limitating decisions, which are of two parts:  
    1. Some observations were left out - when deleting some records due to missing values, it also impacted the structure of our data. An obvious example for this is only looking at companies of 2 years and older. In a real-life scenario, it is not likely that we could limit our predictions this way.   
    However, as said before, in my view these companies need to be modelled and analyzed differently, most likely in a lot-less quantifiable way.  
    There were also records dropped due to NAs, when their number was limited. However, we can't rule out that these were structurally different from other observations - unfortunately the available information on the bisnode dataset is very limited.  
    Also, it's unlikely that we could argue to our front-office bankers that we can't model their client, so let's just not extend a loan (but an actual bank always has the possibility of asking for more information)  
    2. Only a small subset of the variables (and of business performance descriptive financials) were used.  
    This is not generally a bad thing until performance is good enough - here, I made this choice due to lack of available metadata and many missing inputs. When a project is of real $ impact, I would likely not remove half of the seemingly useful variables with a stroke of pen if they could improve performance even just by 1%.
    
The second concern is about the main thing regarding predictions - we are after predicting some events which are outside of our current observations.  
Here, the main limitation is the sample selection - we only modeled based on 3 years! And these 3 years are likely not descriptive of many economic (and political) scenarios happening in Hungary in the last 10-20 years, nevertheless of them in the future. These scenarios will be one a driving force of defaults.
This would be a challenging problem to address - if I was running a business where the bottom-line was dependend on the external validty of our default model, I'd built it into production to keep evaluating it live, and have warnings built-in if a recalibration is needed.

