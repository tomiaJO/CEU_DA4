---
title: "DA4 -  Assignment 1"
author: "Tamas Koncz"
date: '2018-02-11'
output:
  html_notebook:
    df_print: paged
  html_document:
    df_print: paged
  pdf_document: default
---

```{r setup, message=FALSE, include=FALSE}
library(data.table)

library(ggplot2)
library(gridExtra)
library(scales)
library(reshape)

library(caret)
library(glmnet)
library(ROCR)

library(skimr)
library(knitr)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)

theme_set(theme_minimal())   # globally set ggplot theme

set.seed(93)
RMSE <- function(x, true_x) sqrt(mean((x - true_x)^2))
ERR_SQ <- function(x, true_x) (x - true_x)^2
```


```{r, include= FALSE}
rm(list=ls())
x <- fread("./../Data/DA4/bisnode_all.csv",
              stringsAsFactors = FALSE)

print(skim(data))
```

```{r, echo= FALSE}
data <- copy(x)
```

```{r, echo= FALSE}
fun_count_na <- function(dt) {
  missing_values <- as.data.table(t(dt[, lapply(.SD, function(x) sum(is.na(x))), .SDcols = names(dt)]),
                                  keep.rownames = TRUE)
  setnames(missing_values, c("variable", "NA.Count"))
  
  return(missing_values[order(-NA.Count)][NA.Count>0])
}
```

### Understanding the dataset  

After simply reading in the dataset, first step is to understand what we are working with.
A key component of data quality is completness, which is defined as having as little not-known values as possible for our variables.

A glimpse into the missing values, order by most NA-s, top 10 below:
```{r}
fun_count_na(data)[1:10]
```
  
  
The approach I'm taking here is the following:   
    - for some key variables, having NAs is not toleratable. As we are going to predict financial performance, a good example of key variables are liabilites or assets. If such variables are missing, the whole observation will be dropped.  
    - for other variables, which are likely less important, if there are too many missing values, I'm simplying not using the variable for prediction. An example could be birth_year (CEO) - dropping these observations could be costly in terms of limiting our working sample too much (for the benefit we could gain), so rather I chose not to exclude these type of variables.  
    
  
An example of removing some of the observations that are missing information in high-significance columns:  
```{r}
data <- data[!is.na(curr_assets) | !is.na(curr_liab)]
data <- data[!(is.na(ind) | ind == "")]
data <- data[!(is.na(region_m) | region_m == "")]
data <- data[!is.na(founded_year)]
```

A short example of removing some of the variables that are likely not useful, given the very large number of NAs:
```{r}
data[, c("D", 
         "COGS", 
         "finished_prod", 
         "exit_year",
         "wages") := NULL]
```

  
The next step to take is to compile the dataset in a format that will be usable for predictions (every predictor in its on row, every observational unit is in its on column).  
  
  
Generally, multiple years of data are used for prediction. The "current year" is when the default happens - and prediction is based on the financials and some other variables in the preceding two years.   
There is a purposeful limitation here - any company that hasn't been active for at least two years will be dropped. This is a significant constraint, but one with a business-minded driver: very young companies will have strongly different performance dynamics that more matures. Hence, in my view, their default prediction should be based on separate models and likely a lot more expert judgement.  
Technically, this was a trade-off of having a good-model for a limited scope versus a likely less-good (as the available predictors would be more shallow) model for a larger scope. I chose the first option.  


```{r, echo= FALSE}
data <- data[, .(comp_id, 
                  year= year, 
                  curr_liab= curr_liab,
                  curr_assets= curr_assets,
                  curr_sales= sales,
                  founded_year = founded_year,
                  industry = ind,
                  region = region_m)]

data_prev <- data[year < 2016,.(comp_id, 
                     year= year + 1, 
                     prev_liab= curr_liab,
                     prev_assets= curr_assets,
                     prev_sales= curr_sales,
                     prev_founded_year = founded_year,
                     prev_industry = industry,
                     prev_region = region)]

#only financials
data_prev2 <- data[year < 2015,.(comp_id, 
                     year= year + 2, 
                     prev_2_liab= curr_liab,
                     prev_2_assets= curr_assets,
                     prev_2_sales= curr_sales)]

data_prev <- merge(x = data_prev, y = data_prev2, by = c("comp_id", "year"), all.x = FALSE, all.y= FALSE) #filters down population. #TODO explain consequences

dt <- merge(x = data, y = data_prev, by = c("comp_id", "year"), all.x = FALSE, all.y= FALSE)

#only use years where the full dataset is available generally
dt <- dt[year != 2011 &
          year != 2012 &
          year != 2016]

rm(data)
rm(data_prev)
rm(data_prev2)
```
  
  
The final dataset I am working with is based of companies and defaults happening between 2013 - 2015.  


```{r, include= FALSE}
fun_replace_na <- function(x) {
  x <- ifelse(is.na(x), 0, x)
  return(x)
}
```

  
The last important step of data cleansing was to:  
    - recode the industry variable to have only two levels (from previously three), by merging the small "Manufacturing" cohort into the larger one
    - some metrics that were left as NAs (see table below) were recoded as 0, as upon inspecting them I made the judgment that these are likely rather true 0-s than missing data points (so observations weren't missing just at random)  
    
    
```{r, echo= FALSE}

fun_count_na(dt)

#fix founded_year
dt[, is_new := ifelse(!is.na(founded_year) & is.na(prev_founded_year), 1, 0)]
dt[, is_dropped := ifelse(is.na(founded_year) & !is.na(prev_founded_year), 1, 0)]

dt[, founded_year := ifelse(is.na(founded_year), prev_founded_year, founded_year)]
dt[, prev_founded_year := NULL]

dt[, industry := ifelse(is.na(industry), prev_industry, industry)]
dt[, prev_industry := NULL]
dt[industry %like% '*Manuf*', industry := "Manufacturing"] # recode & merge due to small number of observations
dt[industry %like% '*hotel*', industry := "Leisure"]

dt[, region := ifelse(is.na(region), prev_region, region)]
dt[, prev_region := NULL]
dt[, region := factor(region, levels= c("West", "Central", "East"))]

cols = names(dt)
dt[, (cols) := lapply(.SD, fun_replace_na), .SDcols = cols]
```

Now it's time to create an indicator for default.  
  
[Definition for default](https://www.investopedia.com/terms/d/default2.asp): "... debtor is unable to meet the legal obligation of debt repayment...". Hence, the first requirement is to have some obligations to begin with.  

The definition of default for this exercise will follow the along this logic: looking at companies who had any liabilities during a year, but their performance deteoriated significantly the next, measured by sales dropping to 0.
This definition is likely to be punitive, and not exactly relating to companies how have defaulted, but there are trade-offs in every decision in the lack-of and clear description for defaulted parties in the data.
Limitation of this approach are:
    - Sales is not a good measure for default. What we would actually be interested in is companies filing for default on court, but given that's not available, we use sales dropping to zero as an indicatory dummy for businesses "closing down".  
    But let's consider i.e. a project firm, that was created for a set of time purposefully - if these firms are not operating any longer those are not real defaults.
    - We don't know what happened to firms that suddenly "exit" the dataset. AS a conservative approach (in terms of defining default scenarios as wide as possible), if a firm is no longer in the data, but had liabilities as of the last records, that will be considered a default.

Code to create the target variable, is_defaulted:
```{r}
dt[, is_defaulted := ifelse((curr_sales == 0 & prev_sales > 0) &
                            (curr_liab > 0 | (prev_liab > 0 & is_dropped == 1))
                            , 1, 0)]
```
  
  
Summary by year (for reference, see: [here](https://www.ksh.hu/docs/hun/xftp/gyor/gaz/gaz1612.pdf) on p4 there is a table of "defaults" by industry):
  
```{r, fig.align='center', echo= FALSE}
default_viz <- dt[, .(count= .N,
       default_count= sum(is_defaulted),
       default_pct= sum(is_defaulted) / .N * 100,
       drop_count= sum(is_dropped),
       new_count= sum(is_new)),
   keyby = .(industry, region, year)]

max_default_pct <- default_viz[, max(default_pct)]
max_count <- default_viz[, max(count)]

scaleFUN <- function(x) sprintf("%.2f", x)

ggplot(data= default_viz) + 
  geom_bar(aes(x= year, y= count, fill = "# of observations"), stat= "identity") +
  geom_point(aes(x= year, y= default_pct * (max_count / max_default_pct), color = "% of defaults"), stat= "identity") +
  geom_line(aes(x= year, y= default_pct * (max_count / max_default_pct), color = "% of defaults"), stat= "identity") +
  facet_grid(industry ~ region) +
  scale_y_continuous(sec.axis = sec_axis(~./(max_count/max_default_pct), name = "% of defaults", labels=scaleFUN), 
                     labels = scales::comma) + 
  scale_color_manual(name = "Legend", values = c("% of defaults" = "darkblue")) + 
  scale_fill_manual(name = "Legend", values = c("# of observations" = "tomato")) + 
  labs(y = "# of observations", x = "Year", 
       title = "Visualizing all companies, with default rates", subtitle = "Breakdown by Region and Industry")

rm(default_viz)
rm(max_default_pct)
rm(max_count)
```
  
  
#### Visual data exploration

Some of the variables (Assets, Liabilities, and Sales) are close to being log-normally distributed:  

```{r, echo = FALSE, fig.align='center', fig.width=6, fig.height=4}
temp <- dt[, .(industry, 
               region, 
               log_liab= log(curr_liab), 
               log_assets= log(curr_assets),
               log_sales= log(curr_sales))]
temp2 <- melt(temp, id.vars = c("industry", "region"))

ggplot(data= temp2, aes(x = value, fill = variable)) + 
  geom_density(alpha=0.25) +
  facet_grid(industry ~ region) + 
  labs(title = "Density diagrams of Assets, Liabilities and Sales", 
       subtitle = "Breakdown by region & industry")

rm(temp)
rm(temp2)
```

These will be Log-transformated, with a little correction term (for log(0) = -Inf):
```{r}
dt[, prev_liab := ifelse(prev_liab>0, prev_liab, 0.1)]
dt[,log_liab := log(prev_liab)]
dt[, prev_2_liab := ifelse(prev_2_liab>0, prev_2_liab, 0.1)]
dt[,log_liab_2 := log(prev_2_liab)]

dt[, prev_assets := ifelse(prev_assets>0, prev_assets, 0.1)]
dt[,log_asset := log(prev_assets)]
dt[, prev_2_assets := ifelse(prev_2_assets>0, prev_2_assets, 0.1)]
dt[,log_asset_2 := log(prev_2_assets)]

dt[, prev_sales := ifelse(prev_sales>0, prev_sales, 0.1)]
dt[,log_sales := log(prev_sales)]
dt[, prev_2_sales := ifelse(prev_2_sales>0, prev_2_sales, 0.1)]
dt[,log_sales_2 := log(prev_2_sales)]
```

Dropping variables which are not appropriate for prediction, given their availability might be only _after-the-fact_.

```{r}
dt[, curr_liab := NULL]
dt[, curr_assets := NULL]
dt[, curr_sales := NULL]

dt[, is_new := NULL]
dt[, is_dropped := NULL]

dt[, comp_id := NULL]
```
  
Company age:  

```{r, echo= FALSE}
dt[, comp_age := year - founded_year]
```


```{r, echo= FALSE}
data_by_comp_age <- dt[ ,.(default_rate = mean(is_defaulted), num_obs = .N), keyby = .(age_category = cut(comp_age, breaks = c(0, 1, 2, 3, 4, 5, 10, 15, 20, 30, Inf), include.lowest = TRUE))]

p1 <- ggplot(data = data_by_comp_age,
       aes(x = age_category, y = default_rate, size = num_obs)) +
  geom_point() +
  ylim(0, NA) +
  labs(y = "Default frequencies", x = "Age Category", title = "Relationship between company age and default rates", subtitle = "(Note: Age buckets are not same length)") +
  theme(plot.subtitle=element_text(face="italic"))
  
```

```{r, echo= FALSE}
#liab
data_by_liab <- dt[ ,.(default_rate = mean(is_defaulted), num_obs = .N), keyby = .(liab_category = cut(log(prev_liab), breaks = c(-1, 0, 1, 2, 3, 5, 8, 9, 10, 11, 13, 20, Inf), include.lowest = TRUE))]

p2 <- ggplot(data = data_by_liab,
       aes(x = liab_category, y = default_rate, size = num_obs)) +
  geom_point() +
  ylim(0, NA) +
  labs(y = "Default frequencies", x = "Liabilities Category", title = "Relationship between log(Liabilities) and default rates", subtitle = "(Note: Buckets are not same length)") +
  theme(plot.subtitle=element_text(face="italic"))
```

```{r, echo= FALSE}
#assets
data_by_asset <- dt[ ,.(default_rate = mean(is_defaulted), num_obs = .N), keyby = .(asset_category = cut(log(prev_assets), breaks = c(-1, 0, 1, 2, 3, 5, 8, 9, 10, 11, 13, 20, Inf), include.lowest = TRUE))]

p3 <- ggplot(data = data_by_asset,
       aes(x = asset_category, y = default_rate, size = num_obs)) +
  geom_point() +
  ylim(0, NA) +
  labs(y = "Default frequencies", x = "Assets Category", title = "Relationship between log(Assets) and default rates", subtitle = "(Note: Buckets are not same length)") +
  theme(plot.subtitle=element_text(face="italic"))
```

```{r, echo = FALSE, fig.align='center', fig.width=15, fig.height=4}
grid.arrange(p1, p2, p3, ncol = 3)
```

Different relationship patterns between default frequencies and the predictors can be observed:  
    1. for Age, the pattern is relatively linear - for simplicity reasons, no extra term will be created to help fit non-linearities in the logistic regression models  
    2. for Liabilities, the dynamics are more complicated. This is likely due to the fact that while more liabilities can be considered a burden, we can observe that larger companies (who tend to hold more liabilities as well in absolute terms) are less likely to suddenly go bust  
    3. For Assets, there is a clear negative relationship between default frequencies and assets, however it is not linear  

Let's address this with the inclusion of some polynomial terms:  

```{r}
#prev log(liab)
dt[, log_liab_sq := log_liab^2]
dt[, log_liab_cub := log_liab^2]

#t-2 log(liab)
dt[, log_liab_2_sq := log_liab_2^2]
dt[, log_liab_2_cub := log_liab_2^2]

#prev log(assets)
dt[, log_asset_sq := log_asset^2]

#t-2 log(assets)
dt[, log_asset_2_sq := log_asset_2^2]
```

```{r, echo = FALSE}
dt[, c("prev_liab", "prev_assets", "prev_sales",
       "prev_2_liab", "prev_2_assets", "prev_2_sales") := NULL]
```
Calculate a couple of YoY change ratios:  

```{r}
dt[, liab_chg := log_liab - log_liab_2]
dt[, liab_chg := ifelse(!is.na(liab_chg), liab_chg, 1)]
dt[, asset_chg := log_asset - log_asset_2]
dt[, asset_chg := ifelse(!is.na(asset_chg), asset_chg, 1)]
dt[, sales_chg := log_sales - log_sales_2]
dt[, sales_chg := ifelse(!is.na(sales_chg), sales_chg, 1)]
```
    
```{r, echo = FALSE}
dt[, is_defaulted:= ifelse(is_defaulted==1, "Yes", "No")]
dt[, is_defaulted := factor(is_defaulted, levels = c("Yes", "No"))]
```

### Prediction: training models
Note before we jump into prediction: deliberately, I used variables only from the year(s) _before_ the default. This is to make sure that only information was used for prediction which is available aprior.


There will be 3 sub-samples used:  
    1. A training set, 60% of the whole dataset, used to train & tune models via 10-fold CV.  
    2. A test set, 30% of the whole dataset, which is used to compare models and pick the best one for our purpose  
    3. A final performance evaulation set, 10% of the whole data, which is to calculate independent performance metrics (as the previous two sets were used for training and picking the mode, we need an "independent" set to measure real expected performance)  
    
    
```{r}
training_ratio <- 0.6

set.seed(93) #for reproducibility
train_indices <- createDataPartition(y = dt[["is_defaulted"]],
                                     times = 1,
                                     p = training_ratio,
                                     list = FALSE)
data_train <- dt[train_indices, ]
data_test <- dt[-train_indices, ]

split_ratio <- 3/4

set.seed(93) #for reproducibility
perf_indices <- createDataPartition(y = data_test[["is_defaulted"]],
                                     times = 1,
                                     p = split_ratio,
                                     list = FALSE)
data_test <- data_test[perf_indices, ]
data_perf <- data_test[-perf_indices, ]
```
  
  
  
Control function, setting 10-fold CV, and telling the used caret-package that we are predicting binary categorizes (classification setup):  

```{r}
train_control <- trainControl(method = "cv",
                              number = 10,
                              classProbs = TRUE,
                              summaryFunction = twoClassSummary)
```

#### Model 1 - basic glm  
  
The first model trained is a simple logit model, with only a subset of variables used.  
  
(For this model, and all others, caret is set to optimize the AUC.)  

```{r}
#base model
set.seed(93)
glm_model <- train(is_defaulted ~ year + industry + region + log_liab + log_liab_2 + log_asset + log_asset_2 + log_sales + log_sales_2,
                   method = "glm",
                   family = "binomial",
                   metric = "ROC",
                   data = data_train,
                   trControl = train_control)

```
  
In total 4 models are being created - after each are done, there performance will be compared.  
  
```{r, echo = FALSE}
glm_prediction_probs <- predict.train(glm_model, newdata = data_test, type= "prob")
test_truth <- data_test[["is_defaulted"]]
```

```{r, echo= FALSE}
fun_calc_auc <- function(prediction_probs, truth) {
  
  rocr_prediction <- prediction(prediction_probs$Yes, truth)
  AUC <- performance(rocr_prediction, "auc")@y.values[[1]]
  
  return(AUC)
}
```

```{r, echo = FALSE}
fun_roc_create <- function(prediction_probs, truth, AUC= NULL, model= NULL) {

  min_prob <- min(prediction_probs$Yes)
  max_prob <- max(prediction_probs$Yes)
  
  thresholds <- seq(0, max_prob, by = (max_prob - min_prob) / 100)
  
  true_positive_rates <- rep(0, length(thresholds))
  false_positive_rates <- rep(0, length(thresholds))
  
  for (ix in 1:length(thresholds)) {
    thr <- thresholds[ix]
    test_prediction <- ifelse(prediction_probs$Yes > thr, "Yes", "No")
    test_prediction <- factor(test_prediction, levels = c("Yes", "No"))
    cm <- as.matrix(confusionMatrix(test_prediction, truth))
    true_positive_rates[ix] <- cm[1, 1] / (cm[1, 1] + cm[2, 1])
    false_positive_rates[ix] <- cm[1, 2] / (cm[1, 2] + cm[2, 2])
  }
  
  manual_roc <- data.table("threshold" = thresholds,
                           "true_positive_rate" = true_positive_rates,
                           "false_positive_rate" = false_positive_rates)
  
  s_title <- ifelse(is.null(model), "", paste("ROC curve for Model:", model, sep = " "))
  s_subtitle <- ifelse(is.null(AUC), "", paste("AUC =", round(AUC, 2), sep = " "))
  
  ggplot(data = manual_roc, 
         aes(x = false_positive_rate, y = true_positive_rate, color = threshold)) +
    geom_point() +
    geom_abline(intercept = 0, slope = 1,  linetype = "dotted", col = "black") +
    labs(title = s_title, subtitle = s_subtitle)
}
```

```{r, echo = FALSE}
fun_calibration_create <- function(prediction_probs, truth, model= NULL) {
  
  truth_numeric <- ifelse(truth == "Yes", 1, 0)
  score <- prediction_probs$Yes
  
  actual_vs_predicted <- data.table(actual = truth_numeric,
                                    predicted = score)
  
  actual_vs_predicted[, score_category := cut(predicted,
                                      seq(0, 0.6, 0.05),
                                      include.lowest = TRUE)]
  
  calibration <- actual_vs_predicted[, .(mean_actual = mean(actual),
                                         mean_predicted = mean(predicted),
                                         num_obs = .N),
                                     keyby = .(score_category)]
  
  s_title <- ifelse(is.null(model), "", paste("Calibration plot for Model:", model, sep = " "))
  
  ggplot(calibration,
         aes(x = mean_actual, y = mean_predicted, size = num_obs)) +
    geom_point() +
    geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
    ylim(0, 1) + xlim(0, 1) +
    labs(title = s_title)
}
```

```{r, echo= FALSE}
glm_AUC <- fun_calc_auc(glm_prediction_probs, test_truth)
glm_ROC <- fun_roc_create(glm_prediction_probs, test_truth, glm_AUC, "glm")
glm_calibration <- fun_calibration_create(glm_prediction_probs, test_truth, "glm")
```
  
  
#### Model 2

```{r}
tune_grid <- expand.grid("alpha" = c(0, 1),
                         "lambda" = seq(0.0001, 0.0002, 0.00001))

set.seed(93)
glmnet_model <- train(is_defaulted ~ .,
                   method = "glmnet",
                   family = "binomial",
                   metric = "ROC",
                   data = data_train,
                   trControl = train_control,
                   tuneGrid = tune_grid,
                   preProcess = c("center", "scale"))

```

```{r}
glmnet_prediction_probs <- predict.train(glmnet_model, newdata = data_test, type= "prob")
```

```{r, echo= FALSE}
glmnet_AUC <- fun_calc_auc(glmnet_prediction_probs, test_truth)
glmnet_ROC <- fun_roc_create(glmnet_prediction_probs, test_truth, glmnet_AUC, "glmnet")
glmnet_calibration <- fun_calibration_create(glmnet_prediction_probs, test_truth, "glmnet")
```

#### Model 3

```{r}
set.seed(93)
rpart_model <- train(is_defaulted ~ .,
                   data = data_train,
                   method = "rpart",
                   tuneLength=20,
                   metric="ROC",
                   trControl = train_control)

```

```{r}
rpart_prediction_probs <- predict.train(rpart_model, newdata = data_test, type= "prob")
```

```{r, echo= FALSE}
rpart_AUC <- fun_calc_auc(rpart_prediction_probs, test_truth)
rpart_ROC <- fun_roc_create(rpart_prediction_probs, test_truth, rpart_AUC, "rpart")
rpart_calibration <- fun_calibration_create(rpart_prediction_probs, test_truth, "rpart")
```

#### Model 4

```{r}
set.seed(93)
treebag_model <- train(is_defaulted ~ .,
                   data = data_train,
                   method = "treebag",
                   metric="ROC",
                   trControl = train_control)

```

```{r}
treebag_prediction_probs <- predict.train(treebag_model, newdata = data_test, type= "prob")
```

```{r, echo= FALSE}
treebag_AUC <- fun_calc_auc(treebag_prediction_probs, test_truth)
treebag_ROC <- fun_roc_create(treebag_prediction_probs, test_truth, treebag_AUC, "treebag")
treebag_calibration <- fun_calibration_create(treebag_prediction_probs, test_truth, "treebag")
```
  
### Model selection based on performance  
  
```{r, fig.height=15, fig.width=15}
grid.arrange(glm_ROC, glm_calibration,
             glmnet_ROC, glmnet_calibration,
             rpart_ROC, rpart_calibration,
             treebag_ROC, treebag_calibration,
             ncol = 2)
```

### Evaluating performance for the final model

### Last thoughts 

#### Concerns: external validty and implication of sample-limitating decisions

## APPENDIX

```{r, eval= FALSE}
ggplot(data= dt, aes(x= factor(is_defaulted), y= log(curr_liab))) + geom_boxplot()
ggplot(data= dt, aes(x= factor(is_defaulted), y= log(prev_liab))) + geom_boxplot()
```

```{r, eval= FALSE}
dt[, liab_growth := ifelse(curr_liab > 0 & prev_liab == 0, 1, curr_liab/prev_liab)]

ggplot(data= dt, aes(x= factor(is_defaulted), y= log(liab_growth))) + geom_boxplot()

```


```{r, eval= FALSE}
#prev liab vs curr liab
ggplot(data = dt, aes(x= prev_liab, y= curr_liab)) + 
  geom_point(alpha=.1) + 
  scale_y_continuous(trans = 'log10',
                        breaks = trans_breaks('log10', function(x) 10^x),
                        labels = trans_format('log10', math_format(10^.x))) + 
  scale_x_continuous(trans = 'log10',
                        breaks = trans_breaks('log10', function(x) 10^x),
                        labels = trans_format('log10', math_format(10^.x)))

#curr assets vs curr liab
ggplot(data = dt, aes(x= curr_assets, y= curr_liab)) + 
  geom_point(alpha=.1) + 
  scale_y_continuous(trans = 'log10',
                        breaks = trans_breaks('log10', function(x) 10^x),
                        labels = trans_format('log10', math_format(10^.x))) + 
  scale_x_continuous(trans = 'log10',
                        breaks = trans_breaks('log10', function(x) 10^x),
                        labels = trans_format('log10', math_format(10^.x)))  +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") 


#curr assets vs curr liab
ggplot(data = dt, aes(x= curr_liab, y= curr_sales)) + 
  geom_point(alpha=.1) + 
  scale_y_continuous(trans = 'log10',
                        breaks = trans_breaks('log10', function(x) 10^x),
                        labels = trans_format('log10', math_format(10^.x))) + 
  scale_x_continuous(trans = 'log10',
                        breaks = trans_breaks('log10', function(x) 10^x),
                        labels = trans_format('log10', math_format(10^.x)))  +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") 

```


