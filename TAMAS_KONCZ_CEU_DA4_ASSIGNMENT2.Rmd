---
title: "DA4 -  Assignment 1"
author: "Tamas Koncz"
date: '2018-02-11'
output:
  html_document:
    df_print: paged
  html_notebook:
    df_print: paged
  pdf_document: default
---

```{r setup, message=FALSE, include=FALSE}
library(data.table)

library(ggplot2)
library(gridExtra)
library(scales)
library(reshape)

library(caret)
library(glmnet)
library(ROCR)

library(skimr)
library(knitr)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)

theme_set(theme_minimal())   # globally set ggplot theme

set.seed(93)
RMSE <- function(x, true_x) sqrt(mean((x - true_x)^2))
ERR_SQ <- function(x, true_x) (x - true_x)^2
```


```{r, include= FALSE}
# rm(list=ls())
# x <- fread("./../Data/DA4/bisnode_all.csv",
#               stringsAsFactors = FALSE)
# 
# print(skim(data))
```

```{r}
data <- copy(x)
```

```{r}
fun_count_na <- function(dt) {
  missing_values <- as.data.table(t(dt[, lapply(.SD, function(x) sum(is.na(x))), .SDcols = names(dt)]),
                                  keep.rownames = TRUE)
  setnames(missing_values, c("variable", "NA.Count"))
  
  return(missing_values[order(-NA.Count)][NA.Count>0])
}
```


Look at missing values in the dataset:
```{r}
fun_count_na(data)

# v<-dcast(data[, c("comp_id", "year")], comp_id ~ year)
# v[,.N]
```
  
Removing observations where curr_assets (or curr_liab) is missing:
```{r}
data <- data[!is.na(curr_assets) | !is.na(curr_liab)]
data <- data[!(is.na(ind) | ind == "")]
data <- data[!(is.na(region_m) | region_m == "")]
```
  
  
Remove some variables that are likely not useful, given the very large number of NAs:
```{r}
data[, c("D", 
         "COGS", 
         "finished_prod", 
         "exit_year",
         "wages") := NULL]


data[sales == 0, .N]

```


```{r, echo= FALSE}
data <- data[, .(comp_id, 
                  year= year, 
                  curr_liab= curr_liab,
                  curr_assets= curr_assets,
                  curr_sales= sales,
                  founded_year = founded_year,
                  industry = ind,
                  region = region_m)]

data_prev <- data[year < 2016,.(comp_id, 
                     year= year + 1, 
                     prev_liab= curr_liab,
                     prev_assets= curr_assets,
                     prev_sales= curr_sales,
                     prev_founded_year = founded_year,
                     prev_industry = industry,
                     prev_region = region)]

#only financials
data_prev2 <- data[year < 2015,.(comp_id, 
                     year= year + 2, 
                     prev_2_liab= curr_liab,
                     prev_2_assets= curr_assets,
                     prev_2_sales= curr_sales)]

data_prev <- merge(x = data_prev, y = data_prev2, by = c("comp_id", "year"), all.x = TRUE) #all.x --> fine with missing T-2 data


dt <- merge(x = data, y = data_prev, by = c("comp_id", "year"), all = TRUE)

#only use years where the full dataset is available generally
dt <- dt[year != 2011 &
          year != 2012 &
          year != 2016]

rm(data)
rm(data_prev)
rm(data_prev2)
```

```{r, include= FALSE}
fun_replace_na <- function(x) {
  x <- ifelse(is.na(x), 0, x)
  return(x)
}
```


Some data handling:
```{r}
fun_count_na(dt)

#fix founded_year
dt[, is_new := ifelse(!is.na(founded_year) & is.na(prev_founded_year), 1, 0)]
dt[, is_dropped := ifelse(is.na(founded_year) & !is.na(prev_founded_year), 1, 0)]

dt[, founded_year := ifelse(is.na(founded_year), prev_founded_year, founded_year)]
dt[, prev_founded_year := NULL]

dt[, industry := ifelse(is.na(industry), prev_industry, industry)]
dt[, prev_industry := NULL]
dt[industry %like% '*Manuf*', industry := "Manufacturing"] # recode & merge due to small number of observations
dt[industry %like% '*hotel*', industry := "Leisure"]

dt[, region := ifelse(is.na(region), prev_region, region)]
dt[, prev_region := NULL]
dt[, region := factor(region, levels= c("West", "Central", "East"))]

fun_count_na(dt)

cols = names(dt)
dt[, (cols) := lapply(.SD, fun_replace_na), .SDcols = cols] # change for founded year

fun_count_na(dt)
```
  
  
[Definition for default](https://www.investopedia.com/terms/d/default2.asp): "... debtor is unable to meet the legal obligation of debt repayment...". Hence, the first requirement is to have some obligations to begin with.  

The definition of default for this exercise will follow the same logic: looking at companies who had any liabilities during a year, but their performance deteoriated significantly the next.

Code to create the target variable, is_defaulted:
```{r}
dt[, is_defaulted := ifelse((curr_liab > 0 & prev_liab > 0) & 
                              (curr_sales == 0 & prev_sales > 0), 
                            1, 0)]

#TODO: explain logic
```

Example of dropped but not defaulted by this logic:
```{r}
head(dt[is_dropped==1 & is_defaulted == 0])

## does this make sense?
```  
  
  
Summary by year (for reference, [here](https://www.ksh.hu/docs/hun/xftp/gyor/gaz/gaz1612.pdf) on p4 there is a table of "defaults" by industry):
  
```{r, fig.align='center', echo= FALSE}
default_viz <- dt[, .(count= .N,
       default_count= sum(is_defaulted),
       default_pct= sum(is_defaulted) / .N * 100,
       drop_count= sum(is_dropped),
       new_count= sum(is_new)),
   keyby = .(industry, region, year)]

max_default_pct <- default_viz[, max(default_pct)]
max_count <- default_viz[, max(count)]

scaleFUN <- function(x) sprintf("%.2f", x)

ggplot(data= default_viz) + 
  geom_bar(aes(x= year, y= count, fill = "# of observations"), stat= "identity") +
  geom_point(aes(x= year, y= default_pct * (max_count / max_default_pct), color = "% of defaults"), stat= "identity") +
  geom_line(aes(x= year, y= default_pct * (max_count / max_default_pct), color = "% of defaults"), stat= "identity") +
  facet_grid(industry ~ region) +
  scale_y_continuous(sec.axis = sec_axis(~./(max_count/max_default_pct), name = "% of defaults", labels=scaleFUN), 
                     labels = scales::comma) + 
  scale_color_manual(name = "Legend", values = c("% of defaults" = "darkblue")) + 
  scale_fill_manual(name = "Legend", values = c("# of observations" = "tomato")) + 
  labs(y = "# of observations", x = "Year", 
       title = "Visualizing all companies, with default rates", subtitle = "Breakdown by Region and Industry")

rm(default_viz)
rm(max_default_pct)
rm(max_count)
```
  
  
Data viz for other variables:  

Both curr_assets & curr_liab will be log transformed:  

```{r, echo = FALSE}
temp <- dt[, .(industry,
                 region,
                 log_liab= log(curr_liab), 
                 log_assets= log(curr_assets))]
temp2 <- melt(temp, id.vars = c("industry", "region"))

ggplot(data= temp2, aes(x = value, fill = variable)) + 
  geom_density(alpha=0.25) +
  facet_grid(industry ~ region)

rm(temp)
rm(temp2)

# p1 <- ggplot(data= data) + 
#   geom_density(aes(x= log(curr_liab)))
# p2 <- ggplot(data= data, aes(x= log(curr_assets))) + geom_density()
# 
# grid.arrange(p1, p2, ncol = 2)
# 
# rm(p1)
# rm(p2)
```


Calculate a couple of YoY change ratios:  

```{r}
names(dt)
```


```{r}
dt[, liab_chg := prev_liab / prev_2_liab]
dt[, asset_chg := prev_assets / prev_2_assets]
dt[, sales_chg := prev_sales / prev_2_sales]
```
  
Company age:  

```{r}
dt[, comp_age := year - founded_year]
dt[, .N, by = founded_year][order(-N)]
```


```{r}
# data_by_days_since_scheduled <- dt[ ,
#   .(default_rate = mean(is_defaulted), num_obs = .N),
#   keyby = .(days_category = cut(days_since_scheduled, breaks = c(-1, 0, 1, 2, 5, 10, 30, Inf), include.lowest = TRUE))]
# 
# ggplot(data = data_by_days_since_scheduled, 
#        aes(x = days_category, y = no_show_rate, size = num_obs)) +
#   geom_point() +
#   ylim(0, NA)
```




```{r}
dt[, is_defaulted:= ifelse(is_defaulted==1, "Yes", "No")]
dt[, is_defaulted := factor(is_defaulted, levels = c("Yes", "No"))]
```


```{r}
training_ratio <- 0.7

set.seed(93) #for reproducibility
train_indices <- createDataPartition(y = dt[["is_defaulted"]],
                                     times = 1,
                                     p = training_ratio,
                                     list = FALSE)
data_train <- dt[train_indices, ]
data_test <- dt[-train_indices, ]
```

```{r}
train_control <- trainControl(method = "cv",
                              number = 10,
                              classProbs = TRUE)
```

#### Model 1

```{r}
#base model
set.seed(93)
glm_model <- train(is_defaulted ~ year + industry + region + prev_liab + prev_assets + prev_2_liab + prev_2_assets,
                   method = "glm",
                   data = data_train,
                   trControl = train_control)

```


```{r}
test_prediction_probs <- predict.train(glm_model, newdata = data_test, type= "prob")
test_truth <- data_test[["is_defaulted"]]

test_prediction_v2 <- ifelse(test_prediction_probs$Yes > 0.04, "Yes", "No")
test_prediction_v2 <- factor(test_prediction_v2, levels = c("Yes", "No"))
confusionMatrix(test_prediction_v2, test_truth)
```


```{r}
thresholds <- seq(0.01, 0.2, by = 0.005)

true_positive_rates <- rep(0, length(thresholds)) 
false_positive_rates <- rep(0, length(thresholds)) 

for (ix in 1:length(thresholds)) {
  thr <- thresholds[ix]
  test_prediction <- ifelse(test_prediction_probs$Yes > thr, "Yes", "No")
  test_prediction <- factor(test_prediction, levels = c("Yes", "No"))
  cm <- as.matrix(confusionMatrix(test_prediction, test_truth))
  true_positive_rates[ix] <- cm[1, 1] / (cm[1, 1] + cm[2, 1])
  false_positive_rates[ix] <- cm[1, 2] / (cm[1, 2] + cm[2, 2])
} 
```

  
ROC:  

```{r}
manual_roc <- data.table("threshold" = thresholds,
                         "true_positive_rate" = true_positive_rates,
                         "false_positive_rate" = false_positive_rates)

ggplot(data = manual_roc, aes(x = false_positive_rate, y = true_positive_rate, color = threshold)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1,  linetype = "dotted", col = "black")
```
  
  
AUC: 

```{r}
rocr_prediction <- prediction(test_prediction_probs$Yes,
                              data_test[["is_defaulted"]])

AUC <- performance(rocr_prediction, "auc")@y.values[[1]]
print(AUC)
```

#### Model 2

## APPENDIX

```{r, eval= FALSE}
ggplot(data= dt, aes(x= factor(is_defaulted), y= log(curr_liab))) + geom_boxplot()
ggplot(data= dt, aes(x= factor(is_defaulted), y= log(prev_liab))) + geom_boxplot()
```

```{r, eval= FALSE}
dt[, liab_growth := ifelse(curr_liab>0 & prev_liab==0, 1, curr_liab/prev_liab)]

ggplot(data= dt, aes(x= factor(is_defaulted), y= log(liab_growth))) + geom_boxplot()

```


```{r, eval= FALSE}
#prev liab vs curr liab
ggplot(data = dt, aes(x= prev_liab, y= curr_liab)) + 
  geom_point(alpha=.1) + 
  scale_y_continuous(trans = 'log10',
                        breaks = trans_breaks('log10', function(x) 10^x),
                        labels = trans_format('log10', math_format(10^.x))) + 
  scale_x_continuous(trans = 'log10',
                        breaks = trans_breaks('log10', function(x) 10^x),
                        labels = trans_format('log10', math_format(10^.x)))

#curr assets vs curr liab
ggplot(data = dt, aes(x= curr_assets, y= curr_liab)) + 
  geom_point(alpha=.1) + 
  scale_y_continuous(trans = 'log10',
                        breaks = trans_breaks('log10', function(x) 10^x),
                        labels = trans_format('log10', math_format(10^.x))) + 
  scale_x_continuous(trans = 'log10',
                        breaks = trans_breaks('log10', function(x) 10^x),
                        labels = trans_format('log10', math_format(10^.x)))  +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") 


#curr assets vs curr liab
ggplot(data = dt, aes(x= curr_liab, y= curr_sales)) + 
  geom_point(alpha=.1) + 
  scale_y_continuous(trans = 'log10',
                        breaks = trans_breaks('log10', function(x) 10^x),
                        labels = trans_format('log10', math_format(10^.x))) + 
  scale_x_continuous(trans = 'log10',
                        breaks = trans_breaks('log10', function(x) 10^x),
                        labels = trans_format('log10', math_format(10^.x)))  +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") 

```


