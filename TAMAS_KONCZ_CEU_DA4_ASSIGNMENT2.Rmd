---
title: "DA4 -  Assignment 1"
author: "Tamas Koncz"
date: '2018-02-11'
output:
  html_document:
    df_print: paged
  html_notebook:
    df_print: paged
  pdf_document: default
---

```{r setup, message=FALSE, include=FALSE}
library(data.table)

library(ggplot2)
library(gridExtra)
library(scales)
library(reshape)

library(caret)
library(glmnet)
library(ROCR)

library(skimr)
library(knitr)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)

theme_set(theme_minimal())   # globally set ggplot theme

set.seed(93)
RMSE <- function(x, true_x) sqrt(mean((x - true_x)^2))
ERR_SQ <- function(x, true_x) (x - true_x)^2
```


```{r, include= FALSE}
# rm(list=ls())
# x <- fread("./../Data/DA4/bisnode_all.csv",
#               stringsAsFactors = FALSE)
# 
# print(skim(data))
```

```{r}
data <- copy(x)
```

```{r}
fun_count_na <- function(dt) {
  missing_values <- as.data.table(t(dt[, lapply(.SD, function(x) sum(is.na(x))), .SDcols = names(dt)]),
                                  keep.rownames = TRUE)
  setnames(missing_values, c("variable", "NA.Count"))
  
  return(missing_values[order(-NA.Count)][NA.Count>0])
}
```


Look at missing values in the dataset:
```{r}
fun_count_na(data)

# v<-dcast(data[, c("comp_id", "year")], comp_id ~ year)
# v[,.N]
```
  
Removing observations where curr_assets (or curr_liab) is missing:
```{r}
data <- data[!is.na(curr_assets) | !is.na(curr_liab)]
data <- data[!(is.na(ind) | ind == "")]
data <- data[!(is.na(region_m) | region_m == "")]
data <- data[!is.na(founded_year)]
```
  
  
Remove some variables that are likely not useful, given the very large number of NAs:
```{r}
data[, c("D", 
         "COGS", 
         "finished_prod", 
         "exit_year",
         "wages") := NULL]
```


```{r, echo= FALSE}
data <- data[, .(comp_id, 
                  year= year, 
                  curr_liab= curr_liab,
                  curr_assets= curr_assets,
                  curr_sales= sales,
                  founded_year = founded_year,
                  industry = ind,
                  region = region_m)]

data_prev <- data[year < 2016,.(comp_id, 
                     year= year + 1, 
                     prev_liab= curr_liab,
                     prev_assets= curr_assets,
                     prev_sales= curr_sales,
                     prev_founded_year = founded_year,
                     prev_industry = industry,
                     prev_region = region)]

#only financials
data_prev2 <- data[year < 2015,.(comp_id, 
                     year= year + 2, 
                     prev_2_liab= curr_liab,
                     prev_2_assets= curr_assets,
                     prev_2_sales= curr_sales)]

data_prev <- merge(x = data_prev, y = data_prev2, by = c("comp_id", "year"), all.x = FALSE, all.y= FALSE) #filters down population. #TODO explain consequences
?merge

dt <- merge(x = data, y = data_prev, by = c("comp_id", "year"), all = TRUE)

#only use years where the full dataset is available generally
dt <- dt[year != 2011 &
          year != 2012 &
          year != 2016]

rm(data)
rm(data_prev)
rm(data_prev2)
```

```{r, include= FALSE}
fun_replace_na <- function(x) {
  x <- ifelse(is.na(x), 0, x)
  return(x)
}
```


```{r, echo= FALSE}
fun_count_na(dt)

#fix founded_year
dt[, is_new := ifelse(!is.na(founded_year) & is.na(prev_founded_year), 1, 0)]
dt[, is_dropped := ifelse(is.na(founded_year) & !is.na(prev_founded_year), 1, 0)]

dt[, founded_year := ifelse(is.na(founded_year), prev_founded_year, founded_year)]
dt[, prev_founded_year := NULL]

dt[, industry := ifelse(is.na(industry), prev_industry, industry)]
dt[, prev_industry := NULL]
dt[industry %like% '*Manuf*', industry := "Manufacturing"] # recode & merge due to small number of observations
dt[industry %like% '*hotel*', industry := "Leisure"]

dt[, region := ifelse(is.na(region), prev_region, region)]
dt[, prev_region := NULL]
dt[, region := factor(region, levels= c("West", "Central", "East"))]

fun_count_na(dt)

cols = names(dt)
dt[, (cols) := lapply(.SD, fun_replace_na), .SDcols = cols] # change for founded year

fun_count_na(dt)
```

After some data handling in the backend (for technical details please see full code in the .rmd file), it's time to create an indicator for default.  
  
[Definition for default](https://www.investopedia.com/terms/d/default2.asp): "... debtor is unable to meet the legal obligation of debt repayment...". Hence, the first requirement is to have some obligations to begin with.  

The definition of default for this exercise will follow the along this logic: looking at companies who had any liabilities during a year, but their performance deteoriated significantly the next, measured by sales dropping to 0.
This definition is likely to be punitive, and not exactly relating to companies how have defaulted, but there are trade-offs in every decision in the lack-of and clear description for defaulted parties in the data.
Limitation of this approach are:
    - Sales is not a good measure for default. What we would actually be interested in is companies filing for default on court, but given that's not available, we use sales dropping to zero as an indicatory dummy for businesses "closing down".  
    But let's consider i.e. a project firm, that was created for a set of time purposefully - if these firms are not operating any longer those are not real defaults.
    - We don't know what happened to firms that suddenly "exit" the dataset. AS a conservative approach (in terms of defining default scenarios as wide as possible), if a firm is no longer in the data, but had liabilities as of the last records, that will be considered a default.

Code to create the target variable, is_defaulted:
```{r}
dt[, is_defaulted := ifelse((curr_sales == 0 & prev_sales > 0) &
                            (curr_liab > 0 | (prev_liab > 0 & is_dropped == 1))
                            , 1, 0)]

#TODO: explain logic
```

Example of dropped but not defaulted by this logic:
```{r}
head(dt[is_dropped==1 & is_defaulted == 0])

## does this make sense?
```  
  
  
Summary by year (for reference, [here](https://www.ksh.hu/docs/hun/xftp/gyor/gaz/gaz1612.pdf) on p4 there is a table of "defaults" by industry):
  
```{r, fig.align='center', echo= FALSE}
default_viz <- dt[, .(count= .N,
       default_count= sum(is_defaulted),
       default_pct= sum(is_defaulted) / .N * 100,
       drop_count= sum(is_dropped),
       new_count= sum(is_new)),
   keyby = .(industry, region, year)]

max_default_pct <- default_viz[, max(default_pct)]
max_count <- default_viz[, max(count)]

scaleFUN <- function(x) sprintf("%.2f", x)

ggplot(data= default_viz) + 
  geom_bar(aes(x= year, y= count, fill = "# of observations"), stat= "identity") +
  geom_point(aes(x= year, y= default_pct * (max_count / max_default_pct), color = "% of defaults"), stat= "identity") +
  geom_line(aes(x= year, y= default_pct * (max_count / max_default_pct), color = "% of defaults"), stat= "identity") +
  facet_grid(industry ~ region) +
  scale_y_continuous(sec.axis = sec_axis(~./(max_count/max_default_pct), name = "% of defaults", labels=scaleFUN), 
                     labels = scales::comma) + 
  scale_color_manual(name = "Legend", values = c("% of defaults" = "darkblue")) + 
  scale_fill_manual(name = "Legend", values = c("# of observations" = "tomato")) + 
  labs(y = "# of observations", x = "Year", 
       title = "Visualizing all companies, with default rates", subtitle = "Breakdown by Region and Industry")

rm(default_viz)
rm(max_default_pct)
rm(max_count)
```
  
  
#### Visual data exploration

Both Assets, Liabilities, and Sales will be log transformed:  

```{r, echo = FALSE}
temp <- dt[, .(industry, 
               region, 
               log_liab= log(curr_liab), 
               log_assets= log(curr_assets),
               log_sales= log(curr_sales))]
temp2 <- melt(temp, id.vars = c("industry", "region"))

ggplot(data= temp2, aes(x = value, fill = variable)) + 
  geom_density(alpha=0.25) +
  facet_grid(industry ~ region)

rm(temp)
rm(temp2)

# p1 <- ggplot(data= data) + 
#   geom_density(aes(x= log(curr_liab)))
# p2 <- ggplot(data= data, aes(x= log(curr_assets))) + geom_density()
# 
# grid.arrange(p1, p2, ncol = 2)
# 
# rm(p1)
# rm(p2)
```

Log transformations:
```{r}
dt[, prev_liab := ifelse(prev_liab>0, prev_liab, 0.1)]
dt[,log_liab := log(prev_liab)]
dt[, prev_2_liab := ifelse(prev_2_liab>0, prev_2_liab, 0.1)]
dt[,log_liab_2 := log(prev_2_liab)]

dt[, prev_assets := ifelse(prev_assets>0, prev_assets, 0.1)]
dt[,log_asset := log(prev_assets)]
dt[, prev_2_assets := ifelse(prev_2_assets>0, prev_2_assets, 0.1)]
dt[,log_asset_2 := log(prev_2_assets)]

dt[, prev_sales := ifelse(prev_sales>0, prev_sales, 0.1)]
dt[,log_sales := log(prev_sales)]
dt[, prev_2_sales := ifelse(prev_2_sales>0, prev_2_sales, 0.1)]
dt[,log_sales_2 := log(prev_2_sales)]
```

Dropping variables which are not appropriate for prediction, given their availability might be only _after-the-fact_.

```{r}
dt[, curr_liab := NULL]
dt[, curr_assets := NULL]
dt[, curr_sales := NULL]

dt[, is_new := NULL]
dt[, is_dropped := NULL]

dt[, comp_id := NULL]
```

Calculate a couple of YoY change ratios:  


```{r}
dt[, liab_chg := log_liab - log_liab_2]
dt[, liab_chg := ifelse(!is.na(liab_chg), liab_chg, 1)]
dt[, asset_chg := log_asset - log_asset_2]
dt[, asset_chg := ifelse(!is.na(asset_chg), asset_chg, 1)]
dt[, sales_chg := log_sales - log_sales_2]
dt[, sales_chg := ifelse(!is.na(sales_chg), sales_chg, 1)]
```
  
Company age:  

```{r, echo= FALSE}
dt[, comp_age := year - founded_year]
```


```{r, echo= FALSE}
data_by_comp_age <- dt[ ,.(default_rate = mean(is_defaulted), num_obs = .N), keyby = .(age_category = cut(comp_age, breaks = c(0, 1, 2, 3, 4, 5, 10, 15, 20, 30, Inf), include.lowest = TRUE))]

ggplot(data = data_by_comp_age,
       aes(x = age_category, y = default_rate, size = num_obs)) +
  geom_point() +
  ylim(0, NA) +
  labs(y = "Default frequencies", x = "Age Category", title = "Relationship between company age and default rates", subtitle = "(Note: Age buckets are not same length)") +
  theme(plot.subtitle=element_text(face="italic"))
  
```

```{r, echo= FALSE}
#liab
data_by_liab <- dt[ ,.(default_rate = mean(is_defaulted), num_obs = .N), keyby = .(liab_category = cut(log(prev_liab), breaks = c(-1, 0, 1, 2, 3, 5, 8, 9, 10, 11, 13, 20, Inf), include.lowest = TRUE))]

ggplot(data = data_by_liab,
       aes(x = liab_category, y = default_rate, size = num_obs)) +
  geom_point() +
  ylim(0, NA)
```

```{r, echo= FALSE}
#assets
data_by_asset <- dt[ ,.(default_rate = mean(is_defaulted), num_obs = .N), keyby = .(asset_category = cut(log(prev_assets), breaks = c(-1, 0, 1, 2, 3, 5, 8, 9, 10, 11, 13, 20, Inf), include.lowest = TRUE))]

ggplot(data = data_by_asset,
       aes(x = asset_category, y = default_rate, size = num_obs)) +
  geom_point() +
  ylim(0, NA)
```



```{r}
dt[, is_defaulted:= ifelse(is_defaulted==1, "Yes", "No")]
dt[, is_defaulted := factor(is_defaulted, levels = c("Yes", "No"))]
```

#### Prediction: building models
Note before we jump into prediction: deliberately, I used variables only from the year(s) _before_ the default. This is to make sure that only information was used for prediction which is available aprior.

```{r}
training_ratio <- 0.7

set.seed(93) #for reproducibility
train_indices <- createDataPartition(y = dt[["is_defaulted"]],
                                     times = 1,
                                     p = training_ratio,
                                     list = FALSE)
data_train <- dt[train_indices, ]
data_test <- dt[-train_indices, ]
```

```{r}
train_control <- trainControl(method = "cv",
                              number = 10,
                              classProbs = TRUE)
```

#### Model 1

```{r}
#base model
set.seed(93)
glm_model <- train(is_defaulted ~ year + industry + region + prev_liab + prev_assets + prev_2_liab + prev_2_assets,
                   method = "glm",
                   family = "binomial",
                   metric = "AUC",
                   data = data_train,
                   trControl = train_control)

```

```{r}
test_prediction_probs <- predict.train(glm_model, newdata = data_test, type= "prob")
test_truth <- data_test[["is_defaulted"]]
```


AUC: 

```{r}
rocr_prediction <- prediction(test_prediction_probs$Yes,
                              data_test[["is_defaulted"]])

AUC <- performance(rocr_prediction, "auc")@y.values[[1]]
print(AUC)
```

  
ROC:  

```{r}
thresholds <- seq(0.01, 0.2, by = 0.005)

true_positive_rates <- rep(0, length(thresholds))
false_positive_rates <- rep(0, length(thresholds))

for (ix in 1:length(thresholds)) {
  thr <- thresholds[ix]
  test_prediction <- ifelse(test_prediction_probs$Yes > thr, "Yes", "No")
  test_prediction <- factor(test_prediction, levels = c("Yes", "No"))
  cm <- as.matrix(confusionMatrix(test_prediction, test_truth))
  true_positive_rates[ix] <- cm[1, 1] / (cm[1, 1] + cm[2, 1])
  false_positive_rates[ix] <- cm[1, 2] / (cm[1, 2] + cm[2, 2])
}

manual_roc <- data.table("threshold" = thresholds,
                         "true_positive_rate" = true_positive_rates,
                         "false_positive_rate" = false_positive_rates)

ggplot(data = manual_roc, aes(x = false_positive_rate, y = true_positive_rate, color = threshold)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1,  linetype = "dotted", col = "black")
```
  
  
#### Model 2
```{r}
names(dt)

fun_count_na(dt)

dt[is.na(sales_chg)]
```


```{r}
#base model
set.seed(93)
glm_model <- train(is_defaulted ~ .,
                   method = "glmnet",
                   family = "binomial",
                   metric = "AUC",
                   data = data_train,
                   trControl = train_control,
                   tuneLength = 10)

```

```{r}
test_prediction_probs <- predict.train(glm_model, newdata = data_test, type= "prob")
test_truth <- data_test[["is_defaulted"]]
```


AUC: 

```{r}
rocr_prediction <- prediction(test_prediction_probs$Yes,
                              data_test[["is_defaulted"]])

AUC <- performance(rocr_prediction, "auc")@y.values[[1]]
print(AUC)


```
## APPENDIX

```{r, eval= FALSE}
ggplot(data= dt, aes(x= factor(is_defaulted), y= log(curr_liab))) + geom_boxplot()
ggplot(data= dt, aes(x= factor(is_defaulted), y= log(prev_liab))) + geom_boxplot()
```

```{r, eval= FALSE}
dt[, liab_growth := ifelse(curr_liab>0 & prev_liab==0, 1, curr_liab/prev_liab)]

ggplot(data= dt, aes(x= factor(is_defaulted), y= log(liab_growth))) + geom_boxplot()

```


```{r, eval= FALSE}
#prev liab vs curr liab
ggplot(data = dt, aes(x= prev_liab, y= curr_liab)) + 
  geom_point(alpha=.1) + 
  scale_y_continuous(trans = 'log10',
                        breaks = trans_breaks('log10', function(x) 10^x),
                        labels = trans_format('log10', math_format(10^.x))) + 
  scale_x_continuous(trans = 'log10',
                        breaks = trans_breaks('log10', function(x) 10^x),
                        labels = trans_format('log10', math_format(10^.x)))

#curr assets vs curr liab
ggplot(data = dt, aes(x= curr_assets, y= curr_liab)) + 
  geom_point(alpha=.1) + 
  scale_y_continuous(trans = 'log10',
                        breaks = trans_breaks('log10', function(x) 10^x),
                        labels = trans_format('log10', math_format(10^.x))) + 
  scale_x_continuous(trans = 'log10',
                        breaks = trans_breaks('log10', function(x) 10^x),
                        labels = trans_format('log10', math_format(10^.x)))  +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") 


#curr assets vs curr liab
ggplot(data = dt, aes(x= curr_liab, y= curr_sales)) + 
  geom_point(alpha=.1) + 
  scale_y_continuous(trans = 'log10',
                        breaks = trans_breaks('log10', function(x) 10^x),
                        labels = trans_format('log10', math_format(10^.x))) + 
  scale_x_continuous(trans = 'log10',
                        breaks = trans_breaks('log10', function(x) 10^x),
                        labels = trans_format('log10', math_format(10^.x)))  +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") 

```


